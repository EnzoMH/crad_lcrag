"""
ğŸ•·ï¸ ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì„œë¹„ìŠ¤ API

Features:
- í¬ë¡¤ë§ ì‘ì—… ê´€ë¦¬
- ìŠ¤ì¼€ì¤„ë§ ê´€ë¦¬
- ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
- í¬ë¡¤ë§ ì „ëµ ì„¤ì •
"""

import asyncio
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
from enum import Enum

from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks, Query
from pydantic import BaseModel, Field

from aiagent.core import IntelligentCrawlingService, CrawlingAgentIntegration

logger = logging.getLogger(__name__)
router = APIRouter()


# ê¸€ë¡œë²Œ ì˜ì¡´ì„±
async def get_crawling_service() -> IntelligentCrawlingService:
    """í¬ë¡¤ë§ ì„œë¹„ìŠ¤ ì˜ì¡´ì„± - ë©”ì¸ ì•±ì—ì„œ ì£¼ì…"""
    from app import intelligent_crawling_service
    if not intelligent_crawling_service:
        raise HTTPException(status_code=503, detail="í¬ë¡¤ë§ ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    return intelligent_crawling_service


# Enum ì •ì˜
class CrawlingStrategy(str, Enum):
    CONSERVATIVE = "conservative"
    BALANCED = "balanced"
    AGGRESSIVE = "aggressive"
    AI_ADAPTIVE = "ai_adaptive"


class JobStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


# Pydantic ëª¨ë¸ë“¤
class CrawlingTarget(BaseModel):
    """í¬ë¡¤ë§ ëŒ€ìƒ"""
    name: str = Field(..., description="ê¸°ê´€/ëŒ€ìƒ ì´ë¦„")
    category: str = Field(..., description="ì¹´í…Œê³ ë¦¬ (êµíšŒ, í•™ì›, ë³‘ì› ë“±)")
    region: Optional[str] = Field(None, description="ì§€ì—­")
    address: Optional[str] = Field(None, description="ì£¼ì†Œ")
    keywords: Optional[List[str]] = Field(default_factory=list, description="ê²€ìƒ‰ í‚¤ì›Œë“œ")


class CrawlingJobRequest(BaseModel):
    """í¬ë¡¤ë§ ì‘ì—… ìš”ì²­"""
    name: str = Field(..., description="ì‘ì—… ì´ë¦„")
    targets: List[CrawlingTarget] = Field(..., description="í¬ë¡¤ë§ ëŒ€ìƒ ëª©ë¡")
    strategy: CrawlingStrategy = Field(default=CrawlingStrategy.BALANCED, description="í¬ë¡¤ë§ ì „ëµ")
    max_concurrent: int = Field(default=3, ge=1, le=10, description="ìµœëŒ€ ë™ì‹œ ì²˜ë¦¬ ìˆ˜")
    use_ai: bool = Field(default=True, description="AI ë¶„ì„ ì‚¬ìš© ì—¬ë¶€")
    timeout: int = Field(default=300, ge=60, le=3600, description="ì‘ì—… íƒ€ì„ì•„ì›ƒ (ì´ˆ)")
    priority: int = Field(default=1, ge=1, le=10, description="ì‘ì—… ìš°ì„ ìˆœìœ„")


class ScheduledJobRequest(BaseModel):
    """ìŠ¤ì¼€ì¤„ ì‘ì—… ìš”ì²­"""
    job_request: CrawlingJobRequest = Field(..., description="í¬ë¡¤ë§ ì‘ì—… ìš”ì²­")
    cron_expression: str = Field(..., description="Cron í‘œí˜„ì‹")
    timezone: str = Field(default="Asia/Seoul", description="ì‹œê°„ëŒ€")
    max_runs: Optional[int] = Field(None, description="ìµœëŒ€ ì‹¤í–‰ íšŸìˆ˜")
    end_time: Optional[datetime] = Field(None, description="ì¢…ë£Œ ì‹œê°„")


class JobResponse(BaseModel):
    """ì‘ì—… ì‘ë‹µ"""
    job_id: str
    name: str
    status: JobStatus
    strategy: CrawlingStrategy
    targets_count: int
    created_at: datetime
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    progress: float = Field(default=0.0, description="ì§„í–‰ë¥  (0.0-1.0)")
    results: Optional[Dict[str, Any]] = None
    error: Optional[str] = None


class CrawlingResult(BaseModel):
    """í¬ë¡¤ë§ ê²°ê³¼"""
    target_name: str
    success: bool
    data: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    processing_time: float
    extracted_contacts: int = 0
    validation_score: float = 0.0


class JobProgressResponse(BaseModel):
    """ì‘ì—… ì§„í–‰ ìƒí™©"""
    job_id: str
    status: JobStatus
    progress: float
    current_target: Optional[str] = None
    completed_targets: int = 0
    total_targets: int = 0
    results: List[CrawlingResult] = Field(default_factory=list)
    estimated_completion: Optional[datetime] = None


# í¬ë¡¤ë§ ì‘ì—… ê´€ë¦¬ API
@router.post("/jobs", summary="ìƒˆ í¬ë¡¤ë§ ì‘ì—… ìƒì„±")
async def create_crawling_job(
    job_request: CrawlingJobRequest,
    background_tasks: BackgroundTasks,
    service: IntelligentCrawlingService = Depends(get_crawling_service)
) -> JobResponse:
    """ìƒˆë¡œìš´ í¬ë¡¤ë§ ì‘ì—…ì„ ìƒì„±í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤."""
    try:
        # ì‘ì—… ìƒì„±
        job_id = await service.create_job(
            name=job_request.name,
            targets=[target.dict() for target in job_request.targets],
            config={
                "strategy": job_request.strategy.value,
                "max_concurrent": job_request.max_concurrent,
                "use_ai": job_request.use_ai,
                "timeout": job_request.timeout,
                "priority": job_request.priority
            }
        )
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‘ì—… ì‹¤í–‰
        background_tasks.add_task(
            service.start_job,
            job_id
        )
        
        return JobResponse(
            job_id=job_id,
            name=job_request.name,
            status=JobStatus.PENDING,
            strategy=job_request.strategy,
            targets_count=len(job_request.targets),
            created_at=datetime.now()
        )
        
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì‘ì—… ìƒì„± ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì‘ì—… ìƒì„± ì‹¤íŒ¨: {str(e)}")


@router.get("/jobs", summary="í¬ë¡¤ë§ ì‘ì—… ëª©ë¡ ì¡°íšŒ")
async def list_crawling_jobs(
    status: Optional[JobStatus] = Query(None, description="ìƒíƒœë³„ í•„í„°ë§"),
    limit: int = Query(default=50, ge=1, le=100, description="ìµœëŒ€ ì¡°íšŒ ê°œìˆ˜"),
    offset: int = Query(default=0, ge=0, description="ì˜¤í”„ì…‹"),
    service: IntelligentCrawlingService = Depends(get_crawling_service)
) -> List[JobResponse]:
    """í¬ë¡¤ë§ ì‘ì—… ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        jobs = await service.get_jobs(
            status=status.value if status else None,
            limit=limit,
            offset=offset
        )
        
        return [
            JobResponse(
                job_id=job["job_id"],
                name=job["name"],
                status=JobStatus(job["status"]),
                strategy=CrawlingStrategy(job["strategy"]),
                targets_count=job["targets_count"],
                created_at=job["created_at"],
                started_at=job.get("started_at"),
                completed_at=job.get("completed_at"),
                progress=job.get("progress", 0.0),
                results=job.get("results"),
                error=job.get("error")
            )
            for job in jobs
        ]
        
    except Exception as e:
        logger.error(f"ì‘ì—… ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì‘ì—… ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@router.get("/jobs/{job_id}", summary="íŠ¹ì • í¬ë¡¤ë§ ì‘ì—… ì¡°íšŒ")
async def get_crawling_job(
    job_id: str,
    service: IntelligentCrawlingService = Depends(get_crawling_service)
) -> JobResponse:
    """íŠ¹ì • í¬ë¡¤ë§ ì‘ì—…ì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        job = await service.get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail=f"ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {job_id}")
        
        return JobResponse(
            job_id=job["job_id"],
            name=job["name"],
            status=JobStatus(job["status"]),
            strategy=CrawlingStrategy(job["strategy"]),
            targets_count=job["targets_count"],
            created_at=job["created_at"],
            started_at=job.get("started_at"),
            completed_at=job.get("completed_at"),
            progress=job.get("progress", 0.0),
            results=job.get("results"),
            error=job.get("error")
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì‘ì—… ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì‘ì—… ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@router.get("/jobs/{job_id}/progress", summary="ì‘ì—… ì§„í–‰ ìƒí™© ì¡°íšŒ")
async def get_job_progress(
    job_id: str,
    service: IntelligentCrawlingService = Depends(get_crawling_service)
) -> JobProgressResponse:
    """í¬ë¡¤ë§ ì‘ì—…ì˜ ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™©ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        progress = await service.get_job_progress(job_id)
        if not progress:
            raise HTTPException(status_code=404, detail=f"ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {job_id}")
        
        return JobProgressResponse(
            job_id=job_id,
            status=JobStatus(progress["status"]),
            progress=progress["progress"],
            current_target=progress.get("current_target"),
            completed_targets=progress.get("completed_targets", 0),
            total_targets=progress.get("total_targets", 0),
            results=[
                CrawlingResult(
                    target_name=result["target_name"],
                    success=result["success"],
                    data=result.get("data"),
                    error=result.get("error"),
                    processing_time=result.get("processing_time", 0.0),
                    extracted_contacts=result.get("extracted_contacts", 0),
                    validation_score=result.get("validation_score", 0.0)
                )
                for result in progress.get("results", [])
            ],
            estimated_completion=progress.get("estimated_completion")
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì‘ì—… ì§„í–‰ ìƒí™© ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì§„í–‰ ìƒí™© ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@router.post("/jobs/{job_id}/cancel", summary="í¬ë¡¤ë§ ì‘ì—… ì·¨ì†Œ")
async def cancel_crawling_job(
    job_id: str,
    service: IntelligentCrawlingService = Depends(get_crawling_service)
) -> Dict[str, str]:
    """ì‹¤í–‰ ì¤‘ì¸ í¬ë¡¤ë§ ì‘ì—…ì„ ì·¨ì†Œí•©ë‹ˆë‹¤."""
    try:
        success = await service.cancel_job(job_id)
        if not success:
            raise HTTPException(status_code=404, detail=f"ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì·¨ì†Œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {job_id}")
        
        return {"message": f"ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤: {job_id}"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì‘ì—… ì·¨ì†Œ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì‘ì—… ì·¨ì†Œ ì‹¤íŒ¨: {str(e)}")


@router.delete("/jobs/{job_id}", summary="í¬ë¡¤ë§ ì‘ì—… ì‚­ì œ")
async def delete_crawling_job(
    job_id: str,
    service: IntelligentCrawlingService = Depends(get_crawling_service)
) -> Dict[str, str]:
    """ì™„ë£Œëœ í¬ë¡¤ë§ ì‘ì—…ì„ ì‚­ì œí•©ë‹ˆë‹¤."""
    try:
        success = await service.delete_job(job_id)
        if not success:
            raise HTTPException(status_code=404, detail=f"ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {job_id}")
        
        return {"message": f"ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤: {job_id}"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì‘ì—… ì‚­ì œ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì‘ì—… ì‚­ì œ ì‹¤íŒ¨: {str(e)}")


# ìŠ¤ì¼€ì¤„ë§ ê´€ë¦¬ API
@router.post("/schedules", summary="ìŠ¤ì¼€ì¤„ ì‘ì—… ìƒì„±")
async def create_scheduled_job(
    schedule_request: ScheduledJobRequest,
    service: IntelligentCrawlingService = Depends(get_crawling_service)
) -> Dict[str, str]:
    """ìƒˆë¡œìš´ ìŠ¤ì¼€ì¤„ í¬ë¡¤ë§ ì‘ì—…ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        schedule_id = await service.create_schedule(
            job_config={
                "name": schedule_request.job_request.name,
                "targets": [target.dict() for target in schedule_request.job_request.targets],
                "strategy": schedule_request.job_request.strategy.value,
                "max_concurrent": schedule_request.job_request.max_concurrent,
                "use_ai": schedule_request.job_request.use_ai,
                "timeout": schedule_request.job_request.timeout,
                "priority": schedule_request.job_request.priority
            },
            cron_expression=schedule_request.cron_expression,
            timezone=schedule_request.timezone,
            max_runs=schedule_request.max_runs,
            end_time=schedule_request.end_time
        )
        
        return {
            "message": "ìŠ¤ì¼€ì¤„ ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤",
            "schedule_id": schedule_id
        }
        
    except Exception as e:
        logger.error(f"ìŠ¤ì¼€ì¤„ ì‘ì—… ìƒì„± ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ìŠ¤ì¼€ì¤„ ì‘ì—… ìƒì„± ì‹¤íŒ¨: {str(e)}")


@router.get("/schedules", summary="ìŠ¤ì¼€ì¤„ ì‘ì—… ëª©ë¡ ì¡°íšŒ")
async def list_scheduled_jobs(
    service: IntelligentCrawlingService = Depends(get_crawling_service)
) -> List[Dict[str, Any]]:
    """ë“±ë¡ëœ ìŠ¤ì¼€ì¤„ ì‘ì—… ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        schedules = await service.get_schedules()
        return schedules
        
    except Exception as e:
        logger.error(f"ìŠ¤ì¼€ì¤„ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ìŠ¤ì¼€ì¤„ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@router.delete("/schedules/{schedule_id}", summary="ìŠ¤ì¼€ì¤„ ì‘ì—… ì‚­ì œ")
async def delete_scheduled_job(
    schedule_id: str,
    service: IntelligentCrawlingService = Depends(get_crawling_service)
) -> Dict[str, str]:
    """ìŠ¤ì¼€ì¤„ ì‘ì—…ì„ ì‚­ì œí•©ë‹ˆë‹¤."""
    try:
        success = await service.delete_schedule(schedule_id)
        if not success:
            raise HTTPException(status_code=404, detail=f"ìŠ¤ì¼€ì¤„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {schedule_id}")
        
        return {"message": f"ìŠ¤ì¼€ì¤„ì´ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤: {schedule_id}"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ìŠ¤ì¼€ì¤„ ì‚­ì œ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ìŠ¤ì¼€ì¤„ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")


# ëª¨ë‹ˆí„°ë§ ë° í†µê³„ API
@router.get("/statistics", summary="í¬ë¡¤ë§ í†µê³„ ì¡°íšŒ")
async def get_crawling_statistics(
    days: int = Query(default=7, ge=1, le=90, description="ì¡°íšŒ ê¸°ê°„ (ì¼)"),
    service: IntelligentCrawlingService = Depends(get_crawling_service)
) -> Dict[str, Any]:
    """í¬ë¡¤ë§ ì‘ì—… í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        stats = await service.get_statistics(days=days)
        return {
            "period_days": days,
            "total_jobs": stats.get("total_jobs", 0),
            "completed_jobs": stats.get("completed_jobs", 0),
            "failed_jobs": stats.get("failed_jobs", 0),
            "success_rate": stats.get("success_rate", 0.0),
            "total_targets": stats.get("total_targets", 0),
            "extracted_contacts": stats.get("extracted_contacts", 0),
            "average_processing_time": stats.get("average_processing_time", 0.0),
            "strategy_usage": stats.get("strategy_usage", {}),
            "daily_stats": stats.get("daily_stats", [])
        }
        
    except Exception as e:
        logger.error(f"í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@router.get("/queue/status", summary="ì‘ì—… í ìƒíƒœ ì¡°íšŒ")
async def get_queue_status(
    service: IntelligentCrawlingService = Depends(get_crawling_service)
) -> Dict[str, Any]:
    """í˜„ì¬ ì‘ì—… íì˜ ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        status = await service.get_queue_status()
        return {
            "queue_size": status.get("queue_size", 0),
            "running_jobs": status.get("running_jobs", 0),
            "max_concurrent": status.get("max_concurrent", 5),
            "pending_jobs": status.get("pending_jobs", []),
            "active_jobs": status.get("active_jobs", [])
        }
        
    except Exception as e:
        logger.error(f"í ìƒíƒœ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"í ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


# ì„¤ì • ê´€ë¦¬ API
@router.get("/config", summary="í¬ë¡¤ë§ ì„œë¹„ìŠ¤ ì„¤ì • ì¡°íšŒ")
async def get_service_config(
    service: IntelligentCrawlingService = Depends(get_crawling_service)
) -> Dict[str, Any]:
    """í¬ë¡¤ë§ ì„œë¹„ìŠ¤ì˜ í˜„ì¬ ì„¤ì •ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        config = await service.get_config()
        return config
        
    except Exception as e:
        logger.error(f"ì„¤ì • ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì„¤ì • ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@router.put("/config", summary="í¬ë¡¤ë§ ì„œë¹„ìŠ¤ ì„¤ì • ì—…ë°ì´íŠ¸")
async def update_service_config(
    config_updates: Dict[str, Any],
    service: IntelligentCrawlingService = Depends(get_crawling_service)
) -> Dict[str, str]:
    """í¬ë¡¤ë§ ì„œë¹„ìŠ¤ì˜ ì„¤ì •ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    try:
        await service.update_config(config_updates)
        return {"message": "ì„¤ì •ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤"}
        
    except Exception as e:
        logger.error(f"ì„¤ì • ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì„¤ì • ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")


# ìƒíƒœ í™•ì¸ API
@router.get("/health", summary="í¬ë¡¤ë§ ì„œë¹„ìŠ¤ ê±´ê°• ìƒíƒœ")
async def get_service_health(
    service: IntelligentCrawlingService = Depends(get_crawling_service)
) -> Dict[str, Any]:
    """í¬ë¡¤ë§ ì„œë¹„ìŠ¤ì˜ ê±´ê°• ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    try:
        health = await service.get_health_status()
        return {
            "status": health.get("status", "unknown"),
            "uptime": health.get("uptime", 0),
            "active_jobs": health.get("active_jobs", 0),
            "queue_size": health.get("queue_size", 0),
            "last_error": health.get("last_error"),
            "ai_client_status": health.get("ai_client_status", "unknown"),
            "timestamp": datetime.now()
        }
        
    except Exception as e:
        logger.error(f"ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}") 