"""
ğŸ¯ ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì„œë¹„ìŠ¤

í¬ë¡¤ë§ ì—ì´ì „íŠ¸ í†µí•© ì‹œìŠ¤í…œì˜ ê³ ìˆ˜ì¤€ ì¸í„°í˜ì´ìŠ¤
- ì‚¬ìš©ì ì¹œí™”ì  API ì œê³µ
- í¬ë¡¤ë§ ì‘ì—… ìŠ¤ì¼€ì¤„ë§
- ê²°ê³¼ ê´€ë¦¬ ë° ë¦¬í¬íŒ…
- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í†µí•©
"""

import asyncio
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union, Callable
from dataclasses import dataclass, asdict
from enum import Enum
import json
from pathlib import Path

from loguru import logger

from .crawling_integration import (
    CrawlingAgentIntegration, CrawlingJobConfig, CrawlingJobStatus,
    CrawlingTarget, CrawlingResult, CrawlingStrategy, CrawlingPhase,
    create_crawling_target, create_crawling_job_config
)
from .agent_system import AIAgentSystem
from .performance_monitor import PerformanceMonitor
from .monitoring_integration import MonitoringIntegration
from .gcp_optimization_service import GCPOptimizationService


class ServiceStatus(Enum):
    """ì„œë¹„ìŠ¤ ìƒíƒœ"""
    STOPPED = "stopped"
    STARTING = "starting"
    RUNNING = "running"
    PAUSED = "paused"
    ERROR = "error"


@dataclass
class IntelligentCrawlingConfig:
    """ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì„œë¹„ìŠ¤ ì„¤ì •"""
    service_name: str = "IntelligentCrawlingService"
    max_concurrent_jobs: int = 3            # ìµœëŒ€ ë™ì‹œ ì‘ì—… ìˆ˜
    job_queue_size: int = 100               # ì‘ì—… í í¬ê¸°
    default_strategy: CrawlingStrategy = CrawlingStrategy.BALANCED  # ê¸°ë³¸ ì „ëµ
    auto_cleanup_hours: int = 24            # ìë™ ì •ë¦¬ ì£¼ê¸° (ì‹œê°„)
    enable_scheduling: bool = True          # ìŠ¤ì¼€ì¤„ë§ í™œì„±í™”
    enable_monitoring: bool = True          # ëª¨ë‹ˆí„°ë§ í™œì„±í™”
    enable_optimization: bool = True        # ìµœì í™” í™œì„±í™”
    results_retention_days: int = 30        # ê²°ê³¼ ë³´ê´€ ê¸°ê°„ (ì¼)
    log_level: str = "INFO"                 # ë¡œê·¸ ë ˆë²¨


@dataclass 
class JobSchedule:
    """ì‘ì—… ìŠ¤ì¼€ì¤„ ì •ì˜"""
    schedule_id: str                        # ìŠ¤ì¼€ì¤„ ID
    job_config: CrawlingJobConfig           # ì‘ì—… ì„¤ì •
    cron_expression: str                    # Cron í‘œí˜„ì‹
    enabled: bool = True                    # í™œì„±í™” ì—¬ë¶€
    last_run: Optional[datetime] = None     # ë§ˆì§€ë§‰ ì‹¤í–‰ ì‹œê°„
    next_run: Optional[datetime] = None     # ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„
    run_count: int = 0                      # ì‹¤í–‰ íšŸìˆ˜
    metadata: Dict[str, Any] = None         # ë©”íƒ€ë°ì´í„°


class IntelligentCrawlingService:
    """
    ğŸ¯ ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì„œë¹„ìŠ¤
    
    ê¸°ëŠ¥:
    - ê³ ìˆ˜ì¤€ í¬ë¡¤ë§ API ì œê³µ
    - ì‘ì—… ìŠ¤ì¼€ì¤„ë§ ë° í ê´€ë¦¬
    - ê²°ê³¼ ì €ì¥ ë° ì¡°íšŒ
    - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í†µí•©
    - ìë™ ìµœì í™”
    """
    
    def __init__(
        self,
        agent_system: AIAgentSystem,
        performance_monitor: PerformanceMonitor,
        monitoring_integration: MonitoringIntegration,
        gcp_optimization_service: Optional[GCPOptimizationService] = None,
        config: IntelligentCrawlingConfig = None
    ):
        """
        ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            agent_system: AI ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ
            performance_monitor: ì„±ëŠ¥ ëª¨ë‹ˆí„°
            monitoring_integration: í†µí•© ëª¨ë‹ˆí„°ë§
            gcp_optimization_service: GCP ìµœì í™” ì„œë¹„ìŠ¤
            config: ì„œë¹„ìŠ¤ ì„¤ì •
        """
        self.config = config or IntelligentCrawlingConfig()
        self.status = ServiceStatus.STOPPED
        
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.crawling_integration = CrawlingAgentIntegration(
            agent_system=agent_system,
            performance_monitor=performance_monitor,
            monitoring_integration=monitoring_integration,
            gcp_optimization_service=gcp_optimization_service
        )
        
        # ì‘ì—… ê´€ë¦¬
        self.job_queue: asyncio.Queue = asyncio.Queue(maxsize=self.config.job_queue_size)
        self.active_jobs: Dict[str, CrawlingJobStatus] = {}
        self.completed_jobs: Dict[str, CrawlingJobStatus] = {}
        self.scheduled_jobs: Dict[str, JobSchedule] = {}
        
        # ì„œë¹„ìŠ¤ íƒœìŠ¤í¬
        self.service_tasks: List[asyncio.Task] = []
        self.job_processor_semaphore = asyncio.Semaphore(self.config.max_concurrent_jobs)
        
        # í†µê³„ ë° ë©”íŠ¸ë¦­
        self.service_metrics = {
            "service_start_time": None,
            "total_jobs_processed": 0,
            "successful_jobs": 0,
            "failed_jobs": 0,
            "average_job_duration": 0.0,
            "total_targets_processed": 0,
            "current_queue_size": 0,
            "peak_queue_size": 0
        }
        
        logger.info("ğŸ¯ ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def start_service(self):
        """ì„œë¹„ìŠ¤ ì‹œì‘"""
        if self.status == ServiceStatus.RUNNING:
            logger.warning("âš ï¸ ì„œë¹„ìŠ¤ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤")
            return
        
        try:
            self.status = ServiceStatus.STARTING
            self.service_metrics["service_start_time"] = datetime.now()
            
            # ì‘ì—… ì²˜ë¦¬ê¸° ì‹œì‘
            processor_task = asyncio.create_task(self._job_processor())
            self.service_tasks.append(processor_task)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
            if self.config.enable_scheduling:
                scheduler_task = asyncio.create_task(self._job_scheduler())
                self.service_tasks.append(scheduler_task)
            
            # ëª¨ë‹ˆí„°ë§ ì‹œì‘
            if self.config.enable_monitoring:
                monitoring_task = asyncio.create_task(self._monitoring_loop())
                self.service_tasks.append(monitoring_task)
            
            # ìë™ ì •ë¦¬ ì‹œì‘
            cleanup_task = asyncio.create_task(self._cleanup_loop())
            self.service_tasks.append(cleanup_task)
            
            self.status = ServiceStatus.RUNNING
            logger.info("ğŸš€ ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì„œë¹„ìŠ¤ ì‹œì‘ ì™„ë£Œ")
            
        except Exception as e:
            self.status = ServiceStatus.ERROR
            logger.error(f"âŒ ì„œë¹„ìŠ¤ ì‹œì‘ ì‹¤íŒ¨: {e}")
            raise e
    
    async def stop_service(self):
        """ì„œë¹„ìŠ¤ ì¤‘ì§€"""
        if self.status == ServiceStatus.STOPPED:
            return
        
        try:
            logger.info("ğŸ›‘ ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì„œë¹„ìŠ¤ ì¤‘ì§€ ì¤‘...")
            
            # ì„œë¹„ìŠ¤ íƒœìŠ¤í¬ë“¤ ì¤‘ì§€
            for task in self.service_tasks:
                if not task.done():
                    task.cancel()
                    try:
                        await task
                    except asyncio.CancelledError:
                        pass
            
            self.service_tasks.clear()
            
            # í™œì„± ì‘ì—…ë“¤ ì™„ë£Œ ëŒ€ê¸°
            if self.active_jobs:
                logger.info(f"â³ {len(self.active_jobs)}ê°œ í™œì„± ì‘ì—… ì™„ë£Œ ëŒ€ê¸°...")
                while self.active_jobs:
                    await asyncio.sleep(1)
            
            # í¬ë¡¤ë§ í†µí•© ì‹œìŠ¤í…œ ì •ë¦¬
            await self.crawling_integration.cleanup()
            
            self.status = ServiceStatus.STOPPED
            logger.info("âœ… ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì„œë¹„ìŠ¤ ì¤‘ì§€ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ì„œë¹„ìŠ¤ ì¤‘ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
            self.status = ServiceStatus.ERROR
    
    async def submit_crawling_job(
        self,
        targets: List[Dict[str, Any]],
        strategy: CrawlingStrategy = None,
        job_options: Dict[str, Any] = None
    ) -> str:
        """
        í¬ë¡¤ë§ ì‘ì—… ì œì¶œ
        
        Args:
            targets: í¬ë¡¤ë§ ëŒ€ìƒ ëª©ë¡
            strategy: í¬ë¡¤ë§ ì „ëµ
            job_options: ì‘ì—… ì˜µì…˜
            
        Returns:
            str: ì‘ì—… ID
        """
        try:
            # í¬ë¡¤ë§ ëŒ€ìƒ ê°ì²´ ìƒì„±
            crawling_targets = []
            for target_data in targets:
                target = create_crawling_target(
                    name=target_data["name"],
                    category=target_data.get("category", "ê¸°íƒ€"),
                    existing_data=target_data.get("existing_data", {}),
                    target_urls=target_data.get("target_urls", []),
                    search_keywords=target_data.get("search_keywords", []),
                    priority=target_data.get("priority", 1)
                )
                crawling_targets.append(target)
            
            # ì‘ì—… ì„¤ì • ìƒì„±
            job_config = create_crawling_job_config(
                targets=crawling_targets,
                strategy=strategy or self.config.default_strategy,
                max_concurrent=job_options.get("max_concurrent", 5) if job_options else 5,
                enable_ai_validation=job_options.get("enable_ai_validation", True) if job_options else True,
                enable_enrichment=job_options.get("enable_enrichment", True) if job_options else True,
                enable_optimization=self.config.enable_optimization
            )
            
            # ì‘ì—… íì— ì¶”ê°€
            await self.job_queue.put(job_config)
            
            # í í¬ê¸° í†µê³„ ì—…ë°ì´íŠ¸
            current_size = self.job_queue.qsize()
            self.service_metrics["current_queue_size"] = current_size
            if current_size > self.service_metrics["peak_queue_size"]:
                self.service_metrics["peak_queue_size"] = current_size
            
            logger.info(f"ğŸ“ í¬ë¡¤ë§ ì‘ì—… ì œì¶œ ì™„ë£Œ: {job_config.job_id} ({len(crawling_targets)}ê°œ ëŒ€ìƒ)")
            
            return job_config.job_id
            
        except Exception as e:
            logger.error(f"âŒ í¬ë¡¤ë§ ì‘ì—… ì œì¶œ ì‹¤íŒ¨: {e}")
            raise e
    
    async def schedule_crawling_job(
        self,
        schedule_id: str,
        targets: List[Dict[str, Any]],
        cron_expression: str,
        strategy: CrawlingStrategy = None,
        job_options: Dict[str, Any] = None,
        enabled: bool = True
    ) -> bool:
        """
        í¬ë¡¤ë§ ì‘ì—… ìŠ¤ì¼€ì¤„ ë“±ë¡
        
        Args:
            schedule_id: ìŠ¤ì¼€ì¤„ ID
            targets: í¬ë¡¤ë§ ëŒ€ìƒ ëª©ë¡
            cron_expression: Cron í‘œí˜„ì‹
            strategy: í¬ë¡¤ë§ ì „ëµ
            job_options: ì‘ì—… ì˜µì…˜
            enabled: í™œì„±í™” ì—¬ë¶€
            
        Returns:
            bool: ë“±ë¡ ì„±ê³µ ì—¬ë¶€
        """
        try:
            # í¬ë¡¤ë§ ëŒ€ìƒ ê°ì²´ ìƒì„±
            crawling_targets = []
            for target_data in targets:
                target = create_crawling_target(
                    name=target_data["name"],
                    category=target_data.get("category", "ê¸°íƒ€"),
                    existing_data=target_data.get("existing_data", {}),
                    target_urls=target_data.get("target_urls", []),
                    search_keywords=target_data.get("search_keywords", []),
                    priority=target_data.get("priority", 1)
                )
                crawling_targets.append(target)
            
            # ì‘ì—… ì„¤ì • ìƒì„± (í…œí”Œë¦¿)
            job_config_template = create_crawling_job_config(
                targets=crawling_targets,
                strategy=strategy or self.config.default_strategy,
                max_concurrent=job_options.get("max_concurrent", 5) if job_options else 5,
                enable_ai_validation=job_options.get("enable_ai_validation", True) if job_options else True,
                enable_enrichment=job_options.get("enable_enrichment", True) if job_options else True,
                enable_optimization=self.config.enable_optimization
            )
            
            # ìŠ¤ì¼€ì¤„ ë“±ë¡
            schedule = JobSchedule(
                schedule_id=schedule_id,
                job_config=job_config_template,
                cron_expression=cron_expression,
                enabled=enabled,
                next_run=self._calculate_next_run(cron_expression),
                metadata=job_options or {}
            )
            
            self.scheduled_jobs[schedule_id] = schedule
            
            logger.info(f"ğŸ“… í¬ë¡¤ë§ ì‘ì—… ìŠ¤ì¼€ì¤„ ë“±ë¡: {schedule_id} ({cron_expression})")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ í¬ë¡¤ë§ ì‘ì—… ìŠ¤ì¼€ì¤„ ë“±ë¡ ì‹¤íŒ¨: {e}")
            return False
    
    def get_job_status(self, job_id: str) -> Optional[Dict[str, Any]]:
        """ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
        # í™œì„± ì‘ì—…ì—ì„œ ê²€ìƒ‰
        if job_id in self.active_jobs:
            return asdict(self.active_jobs[job_id])
        
        # ì™„ë£Œëœ ì‘ì—…ì—ì„œ ê²€ìƒ‰
        if job_id in self.completed_jobs:
            return asdict(self.completed_jobs[job_id])
        
        # í¬ë¡¤ë§ í†µí•© ì‹œìŠ¤í…œì—ì„œ ê²€ìƒ‰
        status = self.crawling_integration.get_job_status(job_id)
        if status:
            return asdict(status)
        
        return None
    
    def get_active_jobs(self) -> List[Dict[str, Any]]:
        """í™œì„± ì‘ì—… ëª©ë¡ ì¡°íšŒ"""
        return [asdict(job) for job in self.active_jobs.values()]
    
    def get_recent_jobs(self, limit: int = 10) -> List[Dict[str, Any]]:
        """ìµœê·¼ ì‘ì—… ëª©ë¡ ì¡°íšŒ"""
        recent_jobs = list(self.completed_jobs.values())[-limit:]
        return [asdict(job) for job in recent_jobs]
    
    def get_scheduled_jobs(self) -> List[Dict[str, Any]]:
        """ìŠ¤ì¼€ì¤„ëœ ì‘ì—… ëª©ë¡ ì¡°íšŒ"""
        return [asdict(schedule) for schedule in self.scheduled_jobs.values()]
    
    def get_service_metrics(self) -> Dict[str, Any]:
        """ì„œë¹„ìŠ¤ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        metrics = self.service_metrics.copy()
        metrics.update({
            "service_status": self.status.value,
            "active_jobs_count": len(self.active_jobs),
            "completed_jobs_count": len(self.completed_jobs),
            "scheduled_jobs_count": len(self.scheduled_jobs),
            "current_queue_size": self.job_queue.qsize(),
            "crawling_integration_stats": self.crawling_integration.get_performance_stats()
        })
        return metrics
    
    async def cancel_job(self, job_id: str) -> bool:
        """ì‘ì—… ì·¨ì†Œ"""
        try:
            if job_id in self.active_jobs:
                # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‹¤í–‰ ì¤‘ì¸ ì‘ì—…ì„ ì·¨ì†Œí•˜ëŠ” ë¡œì§ í•„ìš”
                self.active_jobs[job_id].status = "cancelled"
                logger.info(f"ğŸš« ì‘ì—… ì·¨ì†Œ ìš”ì²­: {job_id}")
                return True
            else:
                logger.warning(f"âš ï¸ ì·¨ì†Œí•  ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {job_id}")
                return False
        except Exception as e:
            logger.error(f"âŒ ì‘ì—… ì·¨ì†Œ ì‹¤íŒ¨: {job_id} - {e}")
            return False
    
    async def pause_service(self):
        """ì„œë¹„ìŠ¤ ì¼ì‹œ ì •ì§€"""
        if self.status == ServiceStatus.RUNNING:
            self.status = ServiceStatus.PAUSED
            logger.info("â¸ï¸ ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì„œë¹„ìŠ¤ ì¼ì‹œ ì •ì§€")
    
    async def resume_service(self):
        """ì„œë¹„ìŠ¤ ì¬ê°œ"""
        if self.status == ServiceStatus.PAUSED:
            self.status = ServiceStatus.RUNNING
            logger.info("â–¶ï¸ ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì„œë¹„ìŠ¤ ì¬ê°œ")
    
    async def _job_processor(self):
        """ì‘ì—… ì²˜ë¦¬ê¸° ë£¨í”„"""
        logger.info("ğŸ”„ ì‘ì—… ì²˜ë¦¬ê¸° ì‹œì‘")
        
        while True:
            try:
                if self.status != ServiceStatus.RUNNING:
                    await asyncio.sleep(1)
                    continue
                
                # ì‘ì—… íì—ì„œ ì‘ì—… ê°€ì ¸ì˜¤ê¸°
                try:
                    job_config = await asyncio.wait_for(self.job_queue.get(), timeout=1.0)
                except asyncio.TimeoutError:
                    continue
                
                # ë™ì‹œ ì‹¤í–‰ ì œí•œ
                async with self.job_processor_semaphore:
                    # ì‘ì—… ì‹¤í–‰
                    task = asyncio.create_task(self._execute_job(job_config))
                    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë˜ë„ë¡ í•¨ (await í•˜ì§€ ì•ŠìŒ)
                
                # í í¬ê¸° ì—…ë°ì´íŠ¸
                self.service_metrics["current_queue_size"] = self.job_queue.qsize()
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"âŒ ì‘ì—… ì²˜ë¦¬ê¸° ì˜¤ë¥˜: {e}")
                await asyncio.sleep(5)
        
        logger.info("ğŸ›‘ ì‘ì—… ì²˜ë¦¬ê¸° ì¢…ë£Œ")
    
    async def _execute_job(self, job_config: CrawlingJobConfig):
        """ê°œë³„ ì‘ì—… ì‹¤í–‰"""
        job_id = job_config.job_id
        
        try:
            # í™œì„± ì‘ì—…ìœ¼ë¡œ ë“±ë¡
            job_status = CrawlingJobStatus(
                job_id=job_id,
                status="running",
                total_targets=len(job_config.targets),
                started_at=datetime.now()
            )
            self.active_jobs[job_id] = job_status
            
            logger.info(f"âš¡ ì‘ì—… ì‹¤í–‰ ì‹œì‘: {job_id}")
            
            # í¬ë¡¤ë§ í†µí•© ì‹œìŠ¤í…œì„ í†µí•œ ì‹¤í–‰
            result = await self.crawling_integration.start_intelligent_crawling_job(job_config)
            
            # ê²°ê³¼ ì²˜ë¦¬
            if result.status == "completed":
                self.service_metrics["successful_jobs"] += 1
            else:
                self.service_metrics["failed_jobs"] += 1
            
            self.service_metrics["total_jobs_processed"] += 1
            self.service_metrics["total_targets_processed"] += result.total_targets
            
            # í‰ê·  ì‘ì—… ì‹œê°„ ì—…ë°ì´íŠ¸
            if result.total_processing_time > 0:
                total_duration = self.service_metrics["average_job_duration"] * (self.service_metrics["total_jobs_processed"] - 1)
                total_duration += result.total_processing_time
                self.service_metrics["average_job_duration"] = total_duration / self.service_metrics["total_jobs_processed"]
            
            # ì™„ë£Œëœ ì‘ì—…ìœ¼ë¡œ ì´ë™
            self.completed_jobs[job_id] = result
            if job_id in self.active_jobs:
                del self.active_jobs[job_id]
            
            logger.info(f"âœ… ì‘ì—… ì‹¤í–‰ ì™„ë£Œ: {job_id} (ì„±ê³µë¥ : {result.success_rate:.1%})")
            
        except Exception as e:
            logger.error(f"âŒ ì‘ì—… ì‹¤í–‰ ì‹¤íŒ¨: {job_id} - {e}")
            
            # ì‹¤íŒ¨í•œ ì‘ì—… ì²˜ë¦¬
            if job_id in self.active_jobs:
                self.active_jobs[job_id].status = "failed"
                self.active_jobs[job_id].completed_at = datetime.now()
                
                # ì™„ë£Œëœ ì‘ì—…ìœ¼ë¡œ ì´ë™
                self.completed_jobs[job_id] = self.active_jobs[job_id]
                del self.active_jobs[job_id]
            
            self.service_metrics["failed_jobs"] += 1
            self.service_metrics["total_jobs_processed"] += 1
    
    async def _job_scheduler(self):
        """ì‘ì—… ìŠ¤ì¼€ì¤„ëŸ¬ ë£¨í”„"""
        logger.info("ğŸ“… ì‘ì—… ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘")
        
        while True:
            try:
                if self.status != ServiceStatus.RUNNING:
                    await asyncio.sleep(10)
                    continue
                
                current_time = datetime.now()
                
                # ì‹¤í–‰í•  ìŠ¤ì¼€ì¤„ëœ ì‘ì—… í™•ì¸
                for schedule_id, schedule in self.scheduled_jobs.items():
                    if (schedule.enabled and 
                        schedule.next_run and 
                        current_time >= schedule.next_run):
                        
                        # ìƒˆë¡œìš´ ì‘ì—… ID ìƒì„±
                        import uuid
                        new_job_config = schedule.job_config
                        new_job_config.job_id = f"scheduled_{schedule_id}_{int(time.time())}_{str(uuid.uuid4())[:8]}"
                        
                        # ì‘ì—… íì— ì¶”ê°€
                        try:
                            await self.job_queue.put(new_job_config)
                            
                            # ìŠ¤ì¼€ì¤„ ì •ë³´ ì—…ë°ì´íŠ¸
                            schedule.last_run = current_time
                            schedule.next_run = self._calculate_next_run(schedule.cron_expression)
                            schedule.run_count += 1
                            
                            logger.info(f"ğŸ“… ìŠ¤ì¼€ì¤„ëœ ì‘ì—… ì‹¤í–‰: {schedule_id} -> {new_job_config.job_id}")
                            
                        except asyncio.QueueFull:
                            logger.warning(f"âš ï¸ ì‘ì—… íê°€ ê°€ë“ì°¸, ìŠ¤ì¼€ì¤„ëœ ì‘ì—… ê±´ë„ˆëœ€: {schedule_id}")
                
                # 1ë¶„ë§ˆë‹¤ í™•ì¸
                await asyncio.sleep(60)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"âŒ ì‘ì—… ìŠ¤ì¼€ì¤„ëŸ¬ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(60)
        
        logger.info("ğŸ›‘ ì‘ì—… ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ")
    
    async def _monitoring_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        logger.info("ğŸ“Š ëª¨ë‹ˆí„°ë§ ë£¨í”„ ì‹œì‘")
        
        while True:
            try:
                if self.status != ServiceStatus.RUNNING:
                    await asyncio.sleep(30)
                    continue
                
                # ì„œë¹„ìŠ¤ ìƒíƒœ ëª¨ë‹ˆí„°ë§
                metrics = self.get_service_metrics()
                
                # í í¬ê¸° ëª¨ë‹ˆí„°ë§
                queue_size = self.job_queue.qsize()
                if queue_size > self.config.job_queue_size * 0.8:
                    logger.warning(f"âš ï¸ ì‘ì—… í ì‚¬ìš©ë¥  ë†’ìŒ: {queue_size}/{self.config.job_queue_size}")
                
                # í™œì„± ì‘ì—… ëª¨ë‹ˆí„°ë§
                active_count = len(self.active_jobs)
                if active_count > self.config.max_concurrent_jobs * 0.9:
                    logger.warning(f"âš ï¸ í™œì„± ì‘ì—… ìˆ˜ ë†’ìŒ: {active_count}/{self.config.max_concurrent_jobs}")
                
                # 30ì´ˆë§ˆë‹¤ ëª¨ë‹ˆí„°ë§
                await asyncio.sleep(30)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"âŒ ëª¨ë‹ˆí„°ë§ ë£¨í”„ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(60)
        
        logger.info("ğŸ›‘ ëª¨ë‹ˆí„°ë§ ë£¨í”„ ì¢…ë£Œ")
    
    async def _cleanup_loop(self):
        """ìë™ ì •ë¦¬ ë£¨í”„"""
        logger.info("ğŸ§¹ ìë™ ì •ë¦¬ ë£¨í”„ ì‹œì‘")
        
        while True:
            try:
                if self.status != ServiceStatus.RUNNING:
                    await asyncio.sleep(3600)  # 1ì‹œê°„
                    continue
                
                # ì˜¤ë˜ëœ ì™„ë£Œ ì‘ì—… ì •ë¦¬
                cutoff_time = datetime.now() - timedelta(days=self.config.results_retention_days)
                
                jobs_to_remove = []
                for job_id, job_status in self.completed_jobs.items():
                    if job_status.completed_at and job_status.completed_at < cutoff_time:
                        jobs_to_remove.append(job_id)
                
                for job_id in jobs_to_remove:
                    del self.completed_jobs[job_id]
                
                if jobs_to_remove:
                    logger.info(f"ğŸ§¹ ì˜¤ë˜ëœ ì‘ì—… ì •ë¦¬ ì™„ë£Œ: {len(jobs_to_remove)}ê°œ")
                
                # ì„¤ì •ëœ ì£¼ê¸°ë§ˆë‹¤ ì •ë¦¬ (ê¸°ë³¸ 24ì‹œê°„)
                await asyncio.sleep(self.config.auto_cleanup_hours * 3600)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"âŒ ìë™ ì •ë¦¬ ë£¨í”„ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(3600)
        
        logger.info("ğŸ›‘ ìë™ ì •ë¦¬ ë£¨í”„ ì¢…ë£Œ")
    
    def _calculate_next_run(self, cron_expression: str) -> datetime:
        """Cron í‘œí˜„ì‹ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ ê³„ì‚°"""
        # ê°„ë‹¨í•œ Cron íŒŒì‹± (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” croniter ë“±ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê¶Œì¥)
        # ì˜ˆ: "0 9 * * *" -> ë§¤ì¼ ì˜¤ì „ 9ì‹œ
        
        try:
            # ê¸°ë³¸ì ìœ¼ë¡œ 1ì‹œê°„ í›„ë¡œ ì„¤ì • (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì •í™•í•œ Cron íŒŒì‹± í•„ìš”)
            return datetime.now() + timedelta(hours=1)
        except Exception:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ 1ì‹œê°„ í›„
            return datetime.now() + timedelta(hours=1)


# í¸ì˜ í•¨ìˆ˜ë“¤
async def create_intelligent_crawling_service(
    agent_system: AIAgentSystem,
    performance_monitor: PerformanceMonitor,
    monitoring_integration: MonitoringIntegration,
    gcp_optimization_service: Optional[GCPOptimizationService] = None,
    config: IntelligentCrawlingConfig = None
) -> IntelligentCrawlingService:
    """ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì„œë¹„ìŠ¤ ìƒì„± ë° ì‹œì‘"""
    service = IntelligentCrawlingService(
        agent_system=agent_system,
        performance_monitor=performance_monitor,
        monitoring_integration=monitoring_integration,
        gcp_optimization_service=gcp_optimization_service,
        config=config
    )
    
    await service.start_service()
    return service


def create_crawling_targets_from_dict(targets_data: List[Dict[str, Any]]) -> List[CrawlingTarget]:
    """ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ì—ì„œ í¬ë¡¤ë§ ëŒ€ìƒ ìƒì„±"""
    targets = []
    for data in targets_data:
        target = create_crawling_target(
            name=data["name"],
            category=data.get("category", "ê¸°íƒ€"),
            existing_data=data.get("existing_data", {}),
            target_urls=data.get("target_urls", []),
            search_keywords=data.get("search_keywords", []),
            priority=data.get("priority", 1)
        )
        targets.append(target)
    return targets 