"""
ğŸ”— í¬ë¡¤ë§ ì‹œìŠ¤í…œ - AI ì—ì´ì „íŠ¸ í†µí•© ëª¨ë“ˆ

ê¸°ì¡´ í¬ë¡¤ë§ ì‹œìŠ¤í…œ(main_crawler.py)ê³¼ AI ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ í†µí•©í•˜ì—¬
ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì›Œí¬í”Œë¡œìš°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ê¸°ì¡´ í¬ë¡¤ë§ ì—”ì§„ê³¼ AI ì—ì´ì „íŠ¸ ì—°ê²°
- ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì „ëµ ìˆ˜ë¦½
- ì‹¤ì‹œê°„ ì„±ëŠ¥ ìµœì í™”
- AI ê¸°ë°˜ ë°ì´í„° ê²€ì¦ ë° ë³´ê°•
- í†µí•© ëª¨ë‹ˆí„°ë§ ë° ë¦¬í¬íŒ…
"""

import asyncio
import time
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union, Tuple, Callable
from dataclasses import dataclass, field, asdict
from enum import Enum
from collections import deque
import logging
import concurrent.futures
from pathlib import Path

from loguru import logger

from .agent_system import AIAgentSystem, SystemConfig
from .coordinator import AgentCoordinator, WorkflowDefinition, WorkflowTask, WorkflowType
from .performance_monitor import PerformanceMonitor, MetricType, AlertLevel
from .monitoring_integration import MonitoringIntegration
from .gcp_optimization_service import GCPOptimizationService
from ..agents.contact_agent import ContactAgent, ContactTarget, ContactInfo
from ..agents.search_strategy_agent import SearchStrategyAgent, SearchTarget, SearchMethod
from ..agents.validation_agent import ValidationAgent, ValidationTarget
from ..config.gemini_client import GeminiClient
from ..config.prompt_manager import PromptManager


class CrawlingStrategy(Enum):
    """í¬ë¡¤ë§ ì „ëµ ìœ í˜•"""
    CONSERVATIVE = "conservative"    # ë³´ìˆ˜ì  ì „ëµ (ì•ˆì •ì„± ìš°ì„ )
    BALANCED = "balanced"           # ê· í˜• ì „ëµ (ì„±ëŠ¥/ì•ˆì •ì„± ê· í˜•)
    AGGRESSIVE = "aggressive"       # ì ê·¹ì  ì „ëµ (ì„±ëŠ¥ ìš°ì„ )
    AI_ADAPTIVE = "ai_adaptive"     # AI ì ì‘í˜• ì „ëµ (ìƒí™©ë³„ ìµœì í™”)


class CrawlingPhase(Enum):
    """í¬ë¡¤ë§ ë‹¨ê³„"""
    PLANNING = "planning"           # ì „ëµ ìˆ˜ë¦½
    DISCOVERY = "discovery"         # ëŒ€ìƒ ë°œê²¬
    EXTRACTION = "extraction"       # ë°ì´í„° ì¶”ì¶œ
    VALIDATION = "validation"       # ë°ì´í„° ê²€ì¦
    ENRICHMENT = "enrichment"       # ë°ì´í„° ë³´ê°•
    OPTIMIZATION = "optimization"   # ì„±ëŠ¥ ìµœì í™”


@dataclass
class CrawlingTarget:
    """í¬ë¡¤ë§ ëŒ€ìƒ ì •ì˜"""
    id: str                          # ê³ ìœ  ì‹ë³„ì
    name: str                        # ê¸°ê´€ëª…
    category: str                    # ì¹´í…Œê³ ë¦¬ (êµíšŒ, í•™ì› ë“±)
    existing_data: Dict[str, Any] = field(default_factory=dict)   # ê¸°ì¡´ ë°ì´í„°
    target_urls: List[str] = field(default_factory=list)         # ëŒ€ìƒ URL ëª©ë¡
    search_keywords: List[str] = field(default_factory=list)     # ê²€ìƒ‰ í‚¤ì›Œë“œ
    priority: int = 1                # ìš°ì„ ìˆœìœ„ (1: ë†’ìŒ, 3: ë‚®ìŒ)
    expected_fields: List[str] = field(default_factory=list)     # ì˜ˆìƒ í•„ë“œ
    crawling_config: Dict[str, Any] = field(default_factory=dict) # í¬ë¡¤ë§ ì„¤ì •


@dataclass
class CrawlingResult:
    """í¬ë¡¤ë§ ê²°ê³¼"""
    target_id: str                   # ëŒ€ìƒ ID
    success: bool                    # ì„±ê³µ ì—¬ë¶€
    extracted_data: Dict[str, Any] = field(default_factory=dict) # ì¶”ì¶œëœ ë°ì´í„°
    confidence_score: float = 0.0    # ì‹ ë¢°ë„ ì ìˆ˜
    source_urls: List[str] = field(default_factory=list)        # ì†ŒìŠ¤ URL
    processing_time: float = 0.0     # ì²˜ë¦¬ ì‹œê°„
    validation_results: Dict[str, Any] = field(default_factory=dict) # ê²€ì¦ ê²°ê³¼
    enrichment_results: Dict[str, Any] = field(default_factory=dict) # ë³´ê°• ê²°ê³¼
    errors: List[str] = field(default_factory=list)             # ì˜¤ë¥˜ ëª©ë¡
    metadata: Dict[str, Any] = field(default_factory=dict)      # ë©”íƒ€ë°ì´í„°


@dataclass
class CrawlingJobConfig:
    """í¬ë¡¤ë§ ì‘ì—… ì„¤ì •"""
    job_id: str                      # ì‘ì—… ID
    targets: List[CrawlingTarget]    # í¬ë¡¤ë§ ëŒ€ìƒ ëª©ë¡
    strategy: CrawlingStrategy = CrawlingStrategy.BALANCED       # í¬ë¡¤ë§ ì „ëµ
    max_concurrent: int = 5          # ìµœëŒ€ ë™ì‹œ ì²˜ë¦¬ ìˆ˜
    timeout_per_target: int = 120    # ëŒ€ìƒë‹¹ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
    enable_ai_validation: bool = True # AI ê²€ì¦ í™œì„±í™”
    enable_enrichment: bool = True   # ë°ì´í„° ë³´ê°• í™œì„±í™”
    enable_optimization: bool = True # ì„±ëŠ¥ ìµœì í™” í™œì„±í™”
    retry_failed: bool = True        # ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„
    max_retries: int = 3             # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
    save_intermediate: bool = True   # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
    output_format: str = "json"      # ì¶œë ¥ í˜•ì‹
    custom_settings: Dict[str, Any] = field(default_factory=dict) # ì‚¬ìš©ì ì •ì˜ ì„¤ì •


@dataclass
class CrawlingJobStatus:
    """í¬ë¡¤ë§ ì‘ì—… ìƒíƒœ"""
    job_id: str                      # ì‘ì—… ID
    status: str = "pending"          # ìƒíƒœ
    total_targets: int = 0           # ì „ì²´ ëŒ€ìƒ ìˆ˜
    completed_targets: int = 0       # ì™„ë£Œëœ ëŒ€ìƒ ìˆ˜
    failed_targets: int = 0          # ì‹¤íŒ¨í•œ ëŒ€ìƒ ìˆ˜
    success_rate: float = 0.0        # ì„±ê³µë¥ 
    average_processing_time: float = 0.0 # í‰ê·  ì²˜ë¦¬ ì‹œê°„
    total_processing_time: float = 0.0   # ì´ ì²˜ë¦¬ ì‹œê°„
    started_at: Optional[datetime] = None    # ì‹œì‘ ì‹œê°„
    completed_at: Optional[datetime] = None  # ì™„ë£Œ ì‹œê°„
    current_phase: CrawlingPhase = CrawlingPhase.PLANNING # í˜„ì¬ ë‹¨ê³„
    ai_agent_stats: Dict[str, Any] = field(default_factory=dict) # AI ì—ì´ì „íŠ¸ í†µê³„
    performance_metrics: Dict[str, Any] = field(default_factory=dict) # ì„±ëŠ¥ ë©”íŠ¸ë¦­
    optimization_results: Dict[str, Any] = field(default_factory=dict) # ìµœì í™” ê²°ê³¼
    results: List[CrawlingResult] = field(default_factory=list) # ê²°ê³¼ ëª©ë¡


class CrawlingAgentIntegration:
    """
    ğŸ”— í¬ë¡¤ë§ ì‹œìŠ¤í…œ - AI ì—ì´ì „íŠ¸ í†µí•© í´ë˜ìŠ¤
    
    ê¸°ëŠ¥:
    - ê¸°ì¡´ í¬ë¡¤ë§ ì—”ì§„ê³¼ AI ì—ì´ì „íŠ¸ ì—°ê²°
    - ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    - ì‹¤ì‹œê°„ ì„±ëŠ¥ ìµœì í™”
    - í†µí•© ëª¨ë‹ˆí„°ë§ ë° ë¦¬í¬íŒ…
    """
    
    def __init__(
        self,
        agent_system: AIAgentSystem,
        performance_monitor: PerformanceMonitor,
        monitoring_integration: MonitoringIntegration,
        gcp_optimization_service: Optional[GCPOptimizationService] = None
    ):
        """
        í¬ë¡¤ë§ ì—ì´ì „íŠ¸ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            agent_system: AI ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ
            performance_monitor: ì„±ëŠ¥ ëª¨ë‹ˆí„°
            monitoring_integration: í†µí•© ëª¨ë‹ˆí„°ë§
            gcp_optimization_service: GCP ìµœì í™” ì„œë¹„ìŠ¤
        """
        self.agent_system = agent_system
        self.performance_monitor = performance_monitor
        self.monitoring_integration = monitoring_integration
        self.gcp_optimization_service = gcp_optimization_service
        
        # ì „ìš© ì—ì´ì „íŠ¸ë“¤ ì´ˆê¸°í™”
        self._initialize_specialized_agents()
        
        # ì›Œí¬í”Œë¡œìš° ì •ì˜
        self._setup_crawling_workflows()
        
        # ì‘ì—… ê´€ë¦¬
        self.active_jobs: Dict[str, CrawlingJobStatus] = {}
        self.job_history: deque = deque(maxlen=1000)
        
        # ì„±ëŠ¥ ì¶”ì 
        self.performance_stats = {
            "total_jobs": 0,
            "successful_jobs": 0,
            "failed_jobs": 0,
            "average_job_duration": 0.0,
            "total_targets_processed": 0,
            "ai_agent_usage_stats": {},
            "optimization_improvements": []
        }
        
        # ìŠ¤ë ˆë“œ í’€ ì‹¤í–‰ì (ë ˆê±°ì‹œ í¬ë¡¤ë§ ì‹œìŠ¤í…œ í˜¸ì¶œìš©)
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=8)
        
        logger.info("ğŸ”— í¬ë¡¤ë§ ì—ì´ì „íŠ¸ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_specialized_agents(self):
        """ì „ìš© ì—ì´ì „íŠ¸ë“¤ ì´ˆê¸°í™”"""
        try:
            # ContactAgent ì´ˆê¸°í™”
            contact_config = self.agent_system._create_agent_config(
                name="CrawlingContactAgent",
                description="í¬ë¡¤ë§ í†µí•©ìš© ì—°ë½ì²˜ ì¶”ì¶œ ì—ì´ì „íŠ¸",
                max_concurrent_requests=10,
                request_timeout=60.0
            )
            self.contact_agent = ContactAgent(contact_config)
            
            # SearchStrategyAgent ì´ˆê¸°í™”
            strategy_config = self.agent_system._create_agent_config(
                name="CrawlingStrategyAgent", 
                description="í¬ë¡¤ë§ í†µí•©ìš© ê²€ìƒ‰ ì „ëµ ì—ì´ì „íŠ¸",
                max_concurrent_requests=5,
                request_timeout=30.0
            )
            self.strategy_agent = SearchStrategyAgent(strategy_config)
            
            # ValidationAgent ì´ˆê¸°í™”  
            validation_config = self.agent_system._create_agent_config(
                name="CrawlingValidationAgent",
                description="í¬ë¡¤ë§ í†µí•©ìš© ë°ì´í„° ê²€ì¦ ì—ì´ì „íŠ¸",
                max_concurrent_requests=8,
                request_timeout=45.0
            )
            self.validation_agent = ValidationAgent(validation_config)
            
            logger.info("âœ… ì „ìš© ì—ì´ì „íŠ¸ë“¤ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ì „ìš© ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise e
    
    def _setup_crawling_workflows(self):
        """í¬ë¡¤ë§ ì›Œí¬í”Œë¡œìš° ì„¤ì •"""
        try:
            # ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì›Œí¬í”Œë¡œìš° ì •ì˜
            intelligent_crawling_workflow = WorkflowDefinition(
                id="intelligent_crawling",
                name="ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì›Œí¬í”Œë¡œìš°",
                description="AI ì—ì´ì „íŠ¸ ê¸°ë°˜ ì§€ëŠ¥í˜• í¬ë¡¤ë§ íŒŒì´í”„ë¼ì¸",
                workflow_type=WorkflowType.PIPELINE,
                tasks=[
                    WorkflowTask(
                        id="strategy_planning",
                        agent_type="SearchStrategyAgent",
                        input_data={"operation": "generate_crawling_strategy"},
                        priority=1
                    ),
                    WorkflowTask(
                        id="legacy_crawling",
                        agent_type="LegacyCrawlerProxy",
                        input_data={"operation": "execute_main_crawler"},
                        dependencies=["strategy_planning"],
                        priority=2
                    ),
                    WorkflowTask(
                        id="ai_contact_extraction",
                        agent_type="ContactAgent",
                        input_data={"operation": "extract_and_enrich_contacts"},
                        dependencies=["legacy_crawling"],
                        priority=3
                    ),
                    WorkflowTask(
                        id="data_validation",
                        agent_type="ValidationAgent",
                        input_data={"operation": "validate_crawled_data"},
                        dependencies=["ai_contact_extraction"],
                        priority=4
                    ),
                    WorkflowTask(
                        id="performance_optimization",
                        agent_type="OptimizerAgent",
                        input_data={"operation": "optimize_crawling_performance"},
                        dependencies=["data_validation"],
                        priority=5
                    )
                ],
                global_timeout=1800,  # 30ë¶„
                failure_policy="partial_continue"
            )
            
            # ì›Œí¬í”Œë¡œìš° ë“±ë¡
            self.agent_system.coordinator.workflow_definitions[intelligent_crawling_workflow.id] = intelligent_crawling_workflow
            
            logger.info("âœ… í¬ë¡¤ë§ ì›Œí¬í”Œë¡œìš° ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ í¬ë¡¤ë§ ì›Œí¬í”Œë¡œìš° ì„¤ì • ì‹¤íŒ¨: {e}")
            raise e
    
    async def start_intelligent_crawling_job(
        self,
        config: CrawlingJobConfig
    ) -> CrawlingJobStatus:
        """
        ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì‘ì—… ì‹œì‘
        
        Args:
            config: í¬ë¡¤ë§ ì‘ì—… ì„¤ì •
            
        Returns:
            CrawlingJobStatus: ì‘ì—… ìƒíƒœ
        """
        job_status = CrawlingJobStatus(
            job_id=config.job_id,
            status="running",
            total_targets=len(config.targets),
            started_at=datetime.now(),
            current_phase=CrawlingPhase.PLANNING
        )
        
        self.active_jobs[config.job_id] = job_status
        
        try:
            logger.info(f"ğŸš€ ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì‘ì—… ì‹œì‘: {config.job_id}")
            
            # 1ë‹¨ê³„: AI ê¸°ë°˜ ì „ëµ ìˆ˜ë¦½
            job_status.current_phase = CrawlingPhase.PLANNING
            strategy_result = await self._plan_crawling_strategy(config)
            
            # 2ë‹¨ê³„: ëŒ€ìƒ ë°œê²¬ ë° URL ìµœì í™”
            job_status.current_phase = CrawlingPhase.DISCOVERY
            discovery_result = await self._discover_crawling_targets(config, strategy_result)
            
            # 3ë‹¨ê³„: ì§€ëŠ¥í˜• ë°ì´í„° ì¶”ì¶œ
            job_status.current_phase = CrawlingPhase.EXTRACTION
            extraction_results = await self._execute_intelligent_extraction(config, discovery_result)
            
            # 4ë‹¨ê³„: AI ê¸°ë°˜ ë°ì´í„° ê²€ì¦
            job_status.current_phase = CrawlingPhase.VALIDATION
            validation_results = await self._validate_extracted_data(config, extraction_results)
            
            # 5ë‹¨ê³„: ë°ì´í„° ë³´ê°• ë° ìµœì í™”
            job_status.current_phase = CrawlingPhase.ENRICHMENT
            enrichment_results = await self._enrich_and_optimize_data(config, validation_results)
            
            # 6ë‹¨ê³„: ì„±ëŠ¥ ìµœì í™” ë° ë¦¬í¬íŒ…
            job_status.current_phase = CrawlingPhase.OPTIMIZATION
            optimization_results = await self._optimize_and_report(config, enrichment_results)
            
            # ìµœì¢… ê²°ê³¼ ì •ë¦¬
            job_status.results = optimization_results
            job_status.completed_targets = len([r for r in optimization_results if r.success])
            job_status.failed_targets = len([r for r in optimization_results if not r.success])
            job_status.success_rate = job_status.completed_targets / job_status.total_targets if job_status.total_targets > 0 else 0
            job_status.completed_at = datetime.now()
            job_status.total_processing_time = (job_status.completed_at - job_status.started_at).total_seconds()
            job_status.status = "completed"
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_performance_stats(job_status)
            
            # ì‘ì—… íˆìŠ¤í† ë¦¬ì— ì €ì¥
            self.job_history.append(job_status)
            if config.job_id in self.active_jobs:
                del self.active_jobs[config.job_id]
            
            logger.info(f"âœ… ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì‘ì—… ì™„ë£Œ: {config.job_id} (ì„±ê³µë¥ : {job_status.success_rate:.1%})")
            
            return job_status
            
        except Exception as e:
            job_status.status = "failed"
            job_status.completed_at = datetime.now()
            logger.error(f"âŒ ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì‘ì—… ì‹¤íŒ¨: {config.job_id} - {str(e)}")
            
            # ì‹¤íŒ¨í•œ ì‘ì—…ë„ íˆìŠ¤í† ë¦¬ì— ì €ì¥
            self.job_history.append(job_status)
            if config.job_id in self.active_jobs:
                del self.active_jobs[config.job_id]
            
            return job_status
    
    async def _plan_crawling_strategy(
        self,
        config: CrawlingJobConfig
    ) -> Dict[str, Any]:
        """AI ê¸°ë°˜ í¬ë¡¤ë§ ì „ëµ ìˆ˜ë¦½"""
        try:
            # SearchStrategyAgentë¥¼ í†µí•œ ì „ëµ ìˆ˜ë¦½
            strategy_input = {
                "targets": [
                    {
                        "name": target.name,
                        "category": target.category,
                        "existing_data": target.existing_data,
                        "target_urls": target.target_urls,
                        "search_keywords": target.search_keywords
                    }
                    for target in config.targets
                ],
                "strategy_type": config.strategy.value,
                "constraints": {
                    "max_concurrent": config.max_concurrent,
                    "timeout_per_target": config.timeout_per_target,
                    "enable_ai_validation": config.enable_ai_validation
                }
            }
            
            strategy_result = await self.strategy_agent.process(strategy_input)
            
            if strategy_result.success:
                logger.info(f"âœ… AI í¬ë¡¤ë§ ì „ëµ ìˆ˜ë¦½ ì™„ë£Œ: {len(config.targets)}ê°œ ëŒ€ìƒ")
                return strategy_result.data
            else:
                logger.warning(f"âš ï¸ AI ì „ëµ ìˆ˜ë¦½ ì‹¤íŒ¨, ê¸°ë³¸ ì „ëµ ì‚¬ìš©: {strategy_result.error}")
                return self._generate_fallback_strategy(config)
                
        except Exception as e:
            logger.error(f"âŒ í¬ë¡¤ë§ ì „ëµ ìˆ˜ë¦½ ì˜¤ë¥˜: {str(e)}")
            return self._generate_fallback_strategy(config)
    
    async def _discover_crawling_targets(
        self,
        config: CrawlingJobConfig,
        strategy_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ëŒ€ìƒ ë°œê²¬ ë° URL ìµœì í™”"""
        try:
            discovery_results = []
            
            # ê° ëŒ€ìƒì— ëŒ€í•´ URL ë°œê²¬ ë° ìµœì í™”
            for target in config.targets:
                # ê¸°ì¡´ URL ê²€ì¦ ë° ìƒˆë¡œìš´ URL ë°œê²¬
                target_urls = target.target_urls.copy()
                
                # AI ê¸°ë°˜ ì¶”ê°€ URL ë°œê²¬
                if strategy_result.get("enable_url_discovery", True):
                    additional_urls = await self._discover_additional_urls(target, strategy_result)
                    target_urls.extend(additional_urls)
                
                # URL ìš°ì„ ìˆœìœ„ ì„¤ì •
                prioritized_urls = await self._prioritize_urls(target, target_urls, strategy_result)
                
                discovery_results.append({
                    "target_id": target.id,
                    "original_urls": target.target_urls,
                    "discovered_urls": target_urls,
                    "prioritized_urls": prioritized_urls,
                    "crawling_order": prioritized_urls[:strategy_result.get("max_urls_per_target", 10)]
                })
            
            logger.info(f"âœ… ëŒ€ìƒ ë°œê²¬ ì™„ë£Œ: {len(discovery_results)}ê°œ ëŒ€ìƒ, í‰ê·  {sum(len(r['discovered_urls']) for r in discovery_results)/len(discovery_results):.1f}ê°œ URL")
            
            return {
                "discovery_results": discovery_results,
                "total_urls_found": sum(len(r['discovered_urls']) for r in discovery_results),
                "strategy_applied": strategy_result
            }
            
        except Exception as e:
            logger.error(f"âŒ ëŒ€ìƒ ë°œê²¬ ì˜¤ë¥˜: {str(e)}")
            # ê¸°ë³¸ ëŒ€ìƒ ë°˜í™˜
            return {
                "discovery_results": [
                    {
                        "target_id": target.id,
                        "original_urls": target.target_urls,
                        "discovered_urls": target.target_urls,
                        "prioritized_urls": target.target_urls,
                        "crawling_order": target.target_urls
                    }
                    for target in config.targets
                ],
                "total_urls_found": sum(len(target.target_urls) for target in config.targets),
                "strategy_applied": strategy_result
            }
    
    async def _execute_intelligent_extraction(
        self,
        config: CrawlingJobConfig,
        discovery_result: Dict[str, Any]
    ) -> List[CrawlingResult]:
        """ì§€ëŠ¥í˜• ë°ì´í„° ì¶”ì¶œ ì‹¤í–‰"""
        try:
            extraction_results = []
            
            # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì²˜ë¦¬ ì œí•œ
            semaphore = asyncio.Semaphore(config.max_concurrent)
            
            # ê° ëŒ€ìƒì— ëŒ€í•´ ë³‘ë ¬ ì¶”ì¶œ
            tasks = []
            for target, discovery_info in zip(config.targets, discovery_result["discovery_results"]):
                task = self._extract_target_data_with_semaphore(
                    semaphore, target, discovery_info, config
                )
                tasks.append(task)
            
            # ë³‘ë ¬ ì‹¤í–‰
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ê²°ê³¼ ì²˜ë¦¬
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"âŒ ëŒ€ìƒ ì¶”ì¶œ ì‹¤íŒ¨: {config.targets[i].id} - {str(result)}")
                    extraction_results.append(CrawlingResult(
                        target_id=config.targets[i].id,
                        success=False,
                        errors=[str(result)]
                    ))
                else:
                    extraction_results.append(result)
            
            successful_extractions = len([r for r in extraction_results if r.success])
            logger.info(f"âœ… ì§€ëŠ¥í˜• ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ: {successful_extractions}/{len(extraction_results)}ê°œ ì„±ê³µ")
            
            return extraction_results
            
        except Exception as e:
            logger.error(f"âŒ ì§€ëŠ¥í˜• ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            # ê¸°ë³¸ ì˜¤ë¥˜ ê²°ê³¼ ë°˜í™˜
            return [
                CrawlingResult(
                    target_id=target.id,
                    success=False,
                    errors=[str(e)]
                )
                for target in config.targets
            ]
    
    async def _extract_target_data_with_semaphore(
        self,
        semaphore: asyncio.Semaphore,
        target: CrawlingTarget,
        discovery_info: Dict[str, Any],
        config: CrawlingJobConfig
    ) -> CrawlingResult:
        """ì„¸ë§ˆí¬ì–´ ì œì–´í•˜ì— ê°œë³„ ëŒ€ìƒ ë°ì´í„° ì¶”ì¶œ"""
        async with semaphore:
            return await self._extract_single_target_data(target, discovery_info, config)
    
    async def _extract_single_target_data(
        self,
        target: CrawlingTarget,
        discovery_info: Dict[str, Any],
        config: CrawlingJobConfig
    ) -> CrawlingResult:
        """ê°œë³„ ëŒ€ìƒ ë°ì´í„° ì¶”ì¶œ"""
        start_time = time.time()
        
        try:
            result = CrawlingResult(
                target_id=target.id,
                success=False
            )
            
            # 1. ë ˆê±°ì‹œ í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì‹¤í–‰ (main_crawler.py ì‹œë®¬ë ˆì´ì…˜)
            legacy_data = await self._execute_legacy_crawler(target, discovery_info)
            
            # 2. AI ê¸°ë°˜ ì—°ë½ì²˜ ì¶”ì¶œ ë° ë³´ê°•
            ai_contact_data = await self._extract_ai_contacts(target, legacy_data, discovery_info)
            
            # 3. ë°ì´í„° ë³‘í•© ë° ì •ë¦¬
            merged_data = self._merge_extraction_data(legacy_data, ai_contact_data, target.existing_data)
            
            # ê²°ê³¼ ì„¤ì •
            result.success = True
            result.extracted_data = merged_data
            result.source_urls = discovery_info.get("crawling_order", [])
            result.processing_time = time.time() - start_time
            result.confidence_score = self._calculate_confidence_score(merged_data, legacy_data, ai_contact_data)
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ê°œë³„ ëŒ€ìƒ ì¶”ì¶œ ì‹¤íŒ¨: {target.id} - {str(e)}")
            return CrawlingResult(
                target_id=target.id,
                success=False,
                processing_time=time.time() - start_time,
                errors=[str(e)]
            )
    
    async def _execute_legacy_crawler(
        self,
        target: CrawlingTarget,
        discovery_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ë ˆê±°ì‹œ í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì‹¤í–‰ (main_crawler.py í†µí•©)"""
        try:
            # ì‹¤ì œ main_crawler.py í˜¸ì¶œì„ ì‹œë®¬ë ˆì´ì…˜
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” subprocess ë˜ëŠ” direct import ì‚¬ìš©
            
            legacy_result = {
                "name": target.name,
                "category": target.category,
                "phone": None,
                "mobile": None,
                "fax": None,
                "email": None,
                "homepage": None,
                "address": None,
                "crawled_urls": discovery_info.get("crawling_order", [])[:3],
                "crawling_method": "py_cpuinfo_optimized",
                "processing_time": 2.5,
                "success": True
            }
            
            # ë¹„ë™ê¸° ì‹¤í–‰ ì‹œë®¬ë ˆì´ì…˜
            await asyncio.sleep(0.1)
            
            logger.debug(f"âœ… ë ˆê±°ì‹œ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì™„ë£Œ: {target.id}")
            return legacy_result
            
        except Exception as e:
            logger.error(f"âŒ ë ˆê±°ì‹œ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹¤íŒ¨: {target.id} - {str(e)}")
            return {"success": False, "error": str(e)}
    
    async def _extract_ai_contacts(
        self,
        target: CrawlingTarget,
        legacy_data: Dict[str, Any],
        discovery_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """AI ê¸°ë°˜ ì—°ë½ì²˜ ì¶”ì¶œ ë° ë³´ê°•"""
        try:
            # ContactAgentë¥¼ í†µí•œ AI ê¸°ë°˜ ì—°ë½ì²˜ ì¶”ì¶œ
            contact_targets = [
                ContactTarget(
                    name=target.name,
                    category=target.category,
                    target_urls=discovery_info.get("crawling_order", [])[:5],
                    search_keywords=target.search_keywords,
                    existing_info=ContactInfo(
                        phone=legacy_data.get("phone"),
                        mobile=legacy_data.get("mobile"),
                        fax=legacy_data.get("fax"),
                        email=legacy_data.get("email"),
                        homepage=legacy_data.get("homepage"),
                        address=legacy_data.get("address")
                    ) if legacy_data.get("success") else None
                )
            ]
            
            contact_input = {
                "targets": contact_targets,
                "extraction_methods": ["web_crawling", "ai_analysis"],
                "verification_required": True,
                "max_sources_per_target": 5
            }
            
            contact_result = await self.contact_agent.process(contact_input)
            
            if contact_result.success and contact_result.data:
                logger.debug(f"âœ… AI ì—°ë½ì²˜ ì¶”ì¶œ ì™„ë£Œ: {target.id}")
                return contact_result.data
            else:
                logger.warning(f"âš ï¸ AI ì—°ë½ì²˜ ì¶”ì¶œ ì‹¤íŒ¨: {target.id}")
                return {"success": False, "error": contact_result.error}
                
        except Exception as e:
            logger.error(f"âŒ AI ì—°ë½ì²˜ ì¶”ì¶œ ì˜¤ë¥˜: {target.id} - {str(e)}")
            return {"success": False, "error": str(e)}
    
    def _merge_extraction_data(
        self,
        legacy_data: Dict[str, Any],
        ai_contact_data: Dict[str, Any],
        existing_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì¶”ì¶œëœ ë°ì´í„° ë³‘í•© ë° ì •ë¦¬"""
        merged_data = existing_data.copy()
        
        # ë ˆê±°ì‹œ ë°ì´í„° ë³‘í•©
        if legacy_data.get("success"):
            for field in ["phone", "mobile", "fax", "email", "homepage", "address"]:
                if legacy_data.get(field) and not merged_data.get(field):
                    merged_data[field] = legacy_data[field]
        
        # AI ì—°ë½ì²˜ ë°ì´í„° ë³‘í•© (ë†’ì€ ì‹ ë¢°ë„ ìš°ì„ )
        if ai_contact_data.get("success") and ai_contact_data.get("results"):
            ai_results = ai_contact_data["results"]
            if ai_results:
                best_result = max(ai_results, key=lambda x: x.get("confidence_score", 0))
                extracted_info = best_result.get("extracted_info", {})
                
                for field in ["phone", "mobile", "fax", "email", "homepage", "address"]:
                    ai_value = extracted_info.get(field)
                    if ai_value and (not merged_data.get(field) or best_result.get("confidence_score", 0) > 0.8):
                        merged_data[field] = ai_value
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        merged_data["_metadata"] = {
            "legacy_crawler_used": legacy_data.get("success", False),
            "ai_agent_used": ai_contact_data.get("success", False),
            "merge_timestamp": datetime.now().isoformat(),
            "data_sources": ["legacy_crawler", "ai_contact_agent"]
        }
        
        return merged_data
    
    def _calculate_confidence_score(
        self,
        merged_data: Dict[str, Any],
        legacy_data: Dict[str, Any],
        ai_contact_data: Dict[str, Any]
    ) -> float:
        """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # ê¸°ë³¸ ì ìˆ˜ (ë°ì´í„° ì™„ì„±ë„)
        required_fields = ["phone", "email", "address"]
        filled_fields = sum(1 for field in required_fields if merged_data.get(field))
        completeness_score = filled_fields / len(required_fields)
        
        # ì†ŒìŠ¤ ì‹ ë¢°ë„ ì ìˆ˜
        source_score = 0.0
        if legacy_data.get("success"):
            source_score += 0.6
        if ai_contact_data.get("success"):
            source_score += 0.4
            # AI ì‹ ë¢°ë„ ì¶”ê°€
            if ai_contact_data.get("results"):
                ai_confidence = max((r.get("confidence_score", 0) for r in ai_contact_data["results"]), default=0)
                source_score += ai_confidence * 0.2
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚°
        score = (completeness_score * 0.6) + (min(source_score, 1.0) * 0.4)
        
        return round(score, 3)
    
    async def _validate_extracted_data(
        self,
        config: CrawlingJobConfig,
        extraction_results: List[CrawlingResult]
    ) -> List[CrawlingResult]:
        """AI ê¸°ë°˜ ë°ì´í„° ê²€ì¦"""
        if not config.enable_ai_validation:
            return extraction_results
        
        try:
            # ValidationAgentë¥¼ í†µí•œ ë°ì´í„° ê²€ì¦
            validation_targets = []
            for result in extraction_results:
                if result.success:
                    validation_targets.append(ValidationTarget(
                        target_id=result.target_id,
                        data_to_validate=result.extracted_data,
                        validation_rules=["phone_format", "email_format", "url_format", "data_consistency"],
                        required_fields=["name", "phone", "email"]
                    ))
            
            if validation_targets:
                validation_input = {
                    "targets": validation_targets,
                    "validation_level": "comprehensive",
                    "auto_correction": True
                }
                
                validation_result = await self.validation_agent.process(validation_input)
                
                if validation_result.success:
                    # ê²€ì¦ ê²°ê³¼ë¥¼ ì›ë³¸ ê²°ê³¼ì— ë³‘í•©
                    validation_data = validation_result.data.get("validation_results", {})
                    
                    for result in extraction_results:
                        if result.target_id in validation_data:
                            target_validation = validation_data[result.target_id]
                            result.validation_results = target_validation
                            
                            # ê²€ì¦ í†µê³¼ ì‹œ ì‹ ë¢°ë„ ì ìˆ˜ í–¥ìƒ
                            if target_validation.get("overall_valid", False):
                                result.confidence_score = min(result.confidence_score * 1.2, 1.0)
                            
                            # ìë™ ìˆ˜ì •ëœ ë°ì´í„° ì ìš©
                            if target_validation.get("corrected_data"):
                                result.extracted_data.update(target_validation["corrected_data"])
                    
                    logger.info(f"âœ… AI ë°ì´í„° ê²€ì¦ ì™„ë£Œ: {len(validation_targets)}ê°œ ëŒ€ìƒ")
                else:
                    logger.warning(f"âš ï¸ AI ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {validation_result.error}")
            
            return extraction_results
            
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ê²€ì¦ ì˜¤ë¥˜: {str(e)}")
            return extraction_results
    
    async def _enrich_and_optimize_data(
        self,
        config: CrawlingJobConfig,
        validation_results: List[CrawlingResult]
    ) -> List[CrawlingResult]:
        """ë°ì´í„° ë³´ê°• ë° ìµœì í™”"""
        if not config.enable_enrichment:
            return validation_results
        
        try:
            # ë°ì´í„° ë³´ê°• ë¡œì§
            for result in validation_results:
                if result.success:
                    # ì¶”ê°€ ë°ì´í„° ë³´ê°• (ì˜ˆ: ì£¼ì†Œ ì •ê·œí™”, ì „í™”ë²ˆí˜¸ í˜•ì‹ í†µì¼ ë“±)
                    enriched_data = await self._enrich_single_result(result.extracted_data)
                    result.enrichment_results = enriched_data
                    
                    # ë³´ê°•ëœ ë°ì´í„°ë¥¼ ì›ë³¸ì— ë³‘í•©
                    if enriched_data.get("success"):
                        result.extracted_data.update(enriched_data.get("enriched_fields", {}))
                        result.confidence_score = min(result.confidence_score * 1.1, 1.0)
            
            successful_enrichments = len([r for r in validation_results if r.enrichment_results.get("success")])
            logger.info(f"âœ… ë°ì´í„° ë³´ê°• ì™„ë£Œ: {successful_enrichments}/{len(validation_results)}ê°œ ì„±ê³µ")
            
            return validation_results
            
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ë³´ê°• ì˜¤ë¥˜: {str(e)}")
            return validation_results
    
    async def _enrich_single_result(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ê°œë³„ ê²°ê³¼ ë°ì´í„° ë³´ê°•"""
        try:
            enriched_fields = {}
            
            # ì „í™”ë²ˆí˜¸ í˜•ì‹ ì •ê·œí™”
            if data.get("phone"):
                normalized_phone = self._normalize_phone_number(data["phone"])
                if normalized_phone != data["phone"]:
                    enriched_fields["phone"] = normalized_phone
            
            # ì´ë©”ì¼ í˜•ì‹ ê²€ì¦ ë° ì •ë¦¬
            if data.get("email"):
                cleaned_email = self._clean_email_address(data["email"])
                if cleaned_email != data["email"]:
                    enriched_fields["email"] = cleaned_email
            
            # í™ˆí˜ì´ì§€ URL ì •ê·œí™”
            if data.get("homepage"):
                normalized_url = self._normalize_url(data["homepage"])
                if normalized_url != data["homepage"]:
                    enriched_fields["homepage"] = normalized_url
            
            return {
                "success": len(enriched_fields) > 0,
                "enriched_fields": enriched_fields,
                "enrichment_count": len(enriched_fields)
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def _optimize_and_report(
        self,
        config: CrawlingJobConfig,
        enrichment_results: List[CrawlingResult]
    ) -> List[CrawlingResult]:
        """ì„±ëŠ¥ ìµœì í™” ë° ë¦¬í¬íŒ…"""
        try:
            # ì„±ëŠ¥ ìµœì í™”
            if config.enable_optimization and self.gcp_optimization_service:
                await self._trigger_performance_optimization(config, enrichment_results)
            
            # ê²°ê³¼ ë¦¬í¬íŒ…
            await self._generate_crawling_report(config, enrichment_results)
            
            return enrichment_results
            
        except Exception as e:
            logger.error(f"âŒ ìµœì í™” ë° ë¦¬í¬íŒ… ì˜¤ë¥˜: {str(e)}")
            return enrichment_results
    
    def _generate_fallback_strategy(self, config: CrawlingJobConfig) -> Dict[str, Any]:
        """ê¸°ë³¸ í¬ë¡¤ë§ ì „ëµ ìƒì„±"""
        return {
            "strategy_type": "conservative",
            "max_urls_per_target": 5,
            "enable_url_discovery": False,
            "parallel_processing": min(config.max_concurrent, 3),
            "timeout_per_url": 30,
            "retry_failed_urls": True,
            "fallback_strategy": True
        }
    
    async def _discover_additional_urls(
        self,
        target: CrawlingTarget,
        strategy_result: Dict[str, Any]
    ) -> List[str]:
        """ì¶”ê°€ URL ë°œê²¬"""
        # ê²€ìƒ‰ í‚¤ì›Œë“œ ê¸°ë°˜ URL ë°œê²¬ ë¡œì§
        additional_urls = []
        
        # êµ¬ê¸€ ê²€ìƒ‰ URL ìƒì„±
        if target.search_keywords:
            search_query = f"{target.name} {' '.join(target.search_keywords[:2])}"
            google_url = f"https://www.google.com/search?q={search_query.replace(' ', '+')}"
            additional_urls.append(google_url)
        
        return additional_urls
    
    async def _prioritize_urls(
        self,
        target: CrawlingTarget,
        urls: List[str],
        strategy_result: Dict[str, Any]
    ) -> List[str]:
        """URL ìš°ì„ ìˆœìœ„ ì„¤ì •"""
        # ê°„ë‹¨í•œ ìš°ì„ ìˆœìœ„ ë¡œì§
        prioritized = []
        
        # 1. ê³µì‹ í™ˆí˜ì´ì§€ ì¶”ì • URLë“¤ ìš°ì„ 
        official_urls = [url for url in urls if any(keyword in url.lower() for keyword in [target.name.lower(), "official", "www"])]
        prioritized.extend(official_urls)
        
        # 2. ë‚˜ë¨¸ì§€ URLë“¤
        remaining_urls = [url for url in urls if url not in official_urls]
        prioritized.extend(remaining_urls)
        
        return prioritized
    
    def _normalize_phone_number(self, phone: str) -> str:
        """ì „í™”ë²ˆí˜¸ ì •ê·œí™”"""
        import re
        # í•œêµ­ ì „í™”ë²ˆí˜¸ í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”
        cleaned = re.sub(r'[^\d]', '', phone)
        if len(cleaned) == 11 and cleaned.startswith('010'):
            return f"{cleaned[:3]}-{cleaned[3:7]}-{cleaned[7:]}"
        elif len(cleaned) == 10 and cleaned.startswith('02'):
            return f"{cleaned[:2]}-{cleaned[2:6]}-{cleaned[6:]}"
        elif len(cleaned) == 11 and cleaned.startswith(('031', '032', '033', '041', '042', '043', '051', '052', '053', '054', '055', '061', '062', '063', '064')):
            return f"{cleaned[:3]}-{cleaned[3:7]}-{cleaned[7:]}"
        return phone
    
    def _clean_email_address(self, email: str) -> str:
        """ì´ë©”ì¼ ì£¼ì†Œ ì •ë¦¬"""
        return email.strip().lower()
    
    def _normalize_url(self, url: str) -> str:
        """URL ì •ê·œí™”"""
        if not url.startswith(('http://', 'https://')):
            return f"https://{url}"
        return url
    
    def _update_performance_stats(self, job_status: CrawlingJobStatus):
        """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.performance_stats["total_jobs"] += 1
        
        if job_status.status == "completed":
            self.performance_stats["successful_jobs"] += 1
        else:
            self.performance_stats["failed_jobs"] += 1
        
        self.performance_stats["total_targets_processed"] += job_status.total_targets
        
        # í‰ê·  ì‘ì—… ì‹œê°„ ì—…ë°ì´íŠ¸
        total_duration = self.performance_stats["average_job_duration"] * (self.performance_stats["total_jobs"] - 1)
        total_duration += job_status.total_processing_time
        self.performance_stats["average_job_duration"] = total_duration / self.performance_stats["total_jobs"]
    
    async def _trigger_performance_optimization(
        self,
        config: CrawlingJobConfig,
        results: List[CrawlingResult]
    ):
        """ì„±ëŠ¥ ìµœì í™” íŠ¸ë¦¬ê±°"""
        try:
            if self.gcp_optimization_service:
                await self.gcp_optimization_service.trigger_optimization_analysis()
                logger.info("âœ… ì„±ëŠ¥ ìµœì í™” íŠ¸ë¦¬ê±° ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ì„±ëŠ¥ ìµœì í™” íŠ¸ë¦¬ê±° ì‹¤íŒ¨: {str(e)}")
    
    async def _generate_crawling_report(
        self,
        config: CrawlingJobConfig,
        results: List[CrawlingResult]
    ):
        """í¬ë¡¤ë§ ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            report = {
                "job_id": config.job_id,
                "summary": {
                    "total_targets": len(results),
                    "successful_targets": len([r for r in results if r.success]),
                    "failed_targets": len([r for r in results if not r.success]),
                    "success_rate": len([r for r in results if r.success]) / len(results) if results else 0,
                    "average_confidence": sum(r.confidence_score for r in results) / len(results) if results else 0,
                    "total_processing_time": sum(r.processing_time for r in results)
                },
                "results": [asdict(result) for result in results],
                "generated_at": datetime.now().isoformat()
            }
            
            # ë¦¬í¬íŠ¸ ì €ì¥ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” íŒŒì¼ ì‹œìŠ¤í…œ ë˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥)
            logger.info(f"ğŸ“Š í¬ë¡¤ë§ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {config.job_id}")
            
        except Exception as e:
            logger.error(f"âŒ í¬ë¡¤ë§ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    def get_job_status(self, job_id: str) -> Optional[CrawlingJobStatus]:
        """ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
        return self.active_jobs.get(job_id)
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ì¡°íšŒ"""
        return self.performance_stats.copy()
    
    def get_recent_jobs(self, limit: int = 10) -> List[CrawlingJobStatus]:
        """ìµœê·¼ ì‘ì—… ëª©ë¡ ì¡°íšŒ"""
        return list(self.job_history)[-limit:]
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # ì‹¤í–‰ì ì¢…ë£Œ
            self.executor.shutdown(wait=True)
            
            # í™œì„± ì‘ì—… ì •ë¦¬
            self.active_jobs.clear()
            
            logger.info("ğŸ§¹ í¬ë¡¤ë§ ì—ì´ì „íŠ¸ í†µí•© ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_crawling_target(
    name: str,
    category: str,
    existing_data: Dict[str, Any] = None,
    target_urls: List[str] = None,
    search_keywords: List[str] = None,
    priority: int = 1
) -> CrawlingTarget:
    """í¬ë¡¤ë§ ëŒ€ìƒ ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    import uuid
    
    return CrawlingTarget(
        id=str(uuid.uuid4()),
        name=name,
        category=category,
        existing_data=existing_data or {},
        target_urls=target_urls or [],
        search_keywords=search_keywords or [],
        priority=priority
    )


def create_crawling_job_config(
    targets: List[CrawlingTarget],
    strategy: CrawlingStrategy = CrawlingStrategy.BALANCED,
    max_concurrent: int = 5,
    enable_ai_validation: bool = True,
    enable_enrichment: bool = True,
    enable_optimization: bool = True
) -> CrawlingJobConfig:
    """í¬ë¡¤ë§ ì‘ì—… ì„¤ì • ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    import uuid
    
    return CrawlingJobConfig(
        job_id=f"crawling_job_{int(time.time())}_{str(uuid.uuid4())[:8]}",
        targets=targets,
        strategy=strategy,
        max_concurrent=max_concurrent,
        enable_ai_validation=enable_ai_validation,
        enable_enrichment=enable_enrichment,
        enable_optimization=enable_optimization
    ) 