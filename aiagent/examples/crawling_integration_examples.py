"""
ğŸ”— í¬ë¡¤ë§ ì‹œìŠ¤í…œ - AI ì—ì´ì „íŠ¸ í†µí•© ì‚¬ìš© ì˜ˆì œ

ê¸°ì¡´ í¬ë¡¤ë§ ì‹œìŠ¤í…œê³¼ AI ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì˜ í†µí•© ì‚¬ìš©ë²•ì„ ë³´ì—¬ì£¼ëŠ” ì˜ˆì œë“¤
- ê¸°ë³¸ ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì‘ì—…
- ëŒ€ëŸ‰ í¬ë¡¤ë§ ì‘ì—… ê´€ë¦¬
- ìŠ¤ì¼€ì¤„ëœ í¬ë¡¤ë§ ì‘ì—…
- ì„±ëŠ¥ ìµœì í™” í†µí•©
- ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
"""

import asyncio
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any
import json

from loguru import logger

from ..core.agent_system import AIAgentSystem, SystemConfig
from ..core.performance_monitor import PerformanceMonitor, PerformanceThresholds, MetricType
from ..core.monitoring_integration import MonitoringIntegration, MonitoringConfig
from ..core.gcp_optimization_service import GCPOptimizationService, OptimizationServiceConfig
from ..core.gcp_optimizer import GCPOptimizer
from ..core.crawling_integration import (
    CrawlingAgentIntegration, CrawlingStrategy, create_crawling_target, create_crawling_job_config
)
from ..core.intelligent_crawling_service import (
    IntelligentCrawlingService, IntelligentCrawlingConfig, create_intelligent_crawling_service
)
from ..config.gemini_client import GeminiClient
from ..config.prompt_manager import PromptManager


class CrawlingIntegrationExamples:
    """í¬ë¡¤ë§ ì‹œìŠ¤í…œ í†µí•© ì˜ˆì œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì˜ˆì œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.examples_run = []
    
    async def run_all_examples(self):
        """ëª¨ë“  ì˜ˆì œ ì‹¤í–‰"""
        logger.info("ğŸš€ í¬ë¡¤ë§ ì‹œìŠ¤í…œ í†µí•© ì˜ˆì œ ì‹œì‘")
        
        examples = [
            ("ê¸°ë³¸ ì§€ëŠ¥í˜• í¬ë¡¤ë§", self.example_basic_intelligent_crawling),
            ("ê³ ê¸‰ í¬ë¡¤ë§ ì „ëµ", self.example_advanced_crawling_strategies),
            ("ëŒ€ëŸ‰ í¬ë¡¤ë§ ê´€ë¦¬", self.example_bulk_crawling_management),
            ("ìŠ¤ì¼€ì¤„ëœ í¬ë¡¤ë§", self.example_scheduled_crawling),
            ("ì„±ëŠ¥ ìµœì í™” í†µí•©", self.example_performance_optimization_integration),
            ("ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§", self.example_real_time_monitoring),
            ("ì¢…í•© ë°ëª¨", self.example_comprehensive_demo)
        ]
        
        for name, example_func in examples:
            try:
                logger.info(f"\nğŸ“‹ {name} ì˜ˆì œ ì‹œì‘")
                await example_func()
                self.examples_run.append(name)
                logger.info(f"âœ… {name} ì˜ˆì œ ì™„ë£Œ")
                await asyncio.sleep(2)
            except Exception as e:
                logger.error(f"âŒ {name} ì˜ˆì œ ì‹¤íŒ¨: {e}")
        
        logger.info(f"\nğŸ‰ í¬ë¡¤ë§ ì‹œìŠ¤í…œ í†µí•© ì˜ˆì œ ì™„ë£Œ ({len(self.examples_run)}/{len(examples)}ê°œ ì„±ê³µ)")
    
    async def example_basic_intelligent_crawling(self):
        """ì˜ˆì œ 1: ê¸°ë³¸ ì§€ëŠ¥í˜• í¬ë¡¤ë§"""
        logger.info("ğŸ¯ ê¸°ë³¸ ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì‹œì‘")
        
        # AI ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        system_config = SystemConfig(
            name="CrawlingIntegrationSystem",
            version="1.0.0",
            max_concurrent_agents=10
        )
        agent_system = AIAgentSystem(system_config)
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„° ì´ˆê¸°í™”
        performance_monitor = PerformanceMonitor()
        await performance_monitor.start_monitoring()
        
        # í†µí•© ëª¨ë‹ˆí„°ë§ ì´ˆê¸°í™”
        monitoring_config = MonitoringConfig(
            enable_agent_monitoring=True,
            enable_system_monitoring=True,
            agent_monitoring_interval=30,
            system_monitoring_interval=60
        )
        monitoring_integration = MonitoringIntegration(
            agent_system=agent_system,
            performance_monitor=performance_monitor,
            config=monitoring_config
        )
        
        # í¬ë¡¤ë§ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        crawling_integration = CrawlingAgentIntegration(
            agent_system=agent_system,
            performance_monitor=performance_monitor,
            monitoring_integration=monitoring_integration
        )
        
        # í¬ë¡¤ë§ ëŒ€ìƒ ì •ì˜
        targets = [
            create_crawling_target(
                name="ì‚¼ì„±êµíšŒ",
                category="êµíšŒ",
                existing_data={"address": "ì„œìš¸ì‹œ ê°•ë‚¨êµ¬"},
                target_urls=["http://samsung.church"],
                search_keywords=["ì‚¼ì„±êµíšŒ", "ê°•ë‚¨êµ¬", "êµíšŒ"]
            ),
            create_crawling_target(
                name="ì˜ì–´ìˆ˜í•™í•™ì›",
                category="í•™ì›",
                existing_data={"phone": "02-123-4567"},
                search_keywords=["ì˜ì–´ìˆ˜í•™í•™ì›", "í•™ì›", "ìˆ˜í•™"]
            ),
            create_crawling_target(
                name="ê°•ë‚¨êµ¬ì²­",
                category="ì£¼ë¯¼ì„¼í„°",
                existing_data={"address": "ì„œìš¸ì‹œ ê°•ë‚¨êµ¬"},
                search_keywords=["ê°•ë‚¨êµ¬ì²­", "êµ¬ì²­", "í–‰ì •"]
            )
        ]
        
        # í¬ë¡¤ë§ ì‘ì—… ì„¤ì •
        job_config = create_crawling_job_config(
            targets=targets,
            strategy=CrawlingStrategy.BALANCED,
            max_concurrent=3,
            enable_ai_validation=True,
            enable_enrichment=True,
            enable_optimization=False
        )
        
        # ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì‹¤í–‰
        result = await crawling_integration.start_intelligent_crawling_job(job_config)
        
        # ê²°ê³¼ ì¶œë ¥
        logger.info(f"ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼:")
        logger.info(f"  - ì‘ì—… ID: {result.job_id}")
        logger.info(f"  - ìƒíƒœ: {result.status}")
        logger.info(f"  - ì´ ëŒ€ìƒ: {result.total_targets}ê°œ")
        logger.info(f"  - ì„±ê³µ: {result.completed_targets}ê°œ")
        logger.info(f"  - ì‹¤íŒ¨: {result.failed_targets}ê°œ")
        logger.info(f"  - ì„±ê³µë¥ : {result.success_rate:.1%}")
        logger.info(f"  - ì²˜ë¦¬ ì‹œê°„: {result.total_processing_time:.1f}ì´ˆ")
        
        # ê°œë³„ ê²°ê³¼ í™•ì¸
        for i, crawling_result in enumerate(result.results):
            logger.info(f"  [{i+1}] {crawling_result.target_id}:")
            logger.info(f"      ì„±ê³µ: {crawling_result.success}")
            logger.info(f"      ì‹ ë¢°ë„: {crawling_result.confidence_score:.3f}")
            logger.info(f"      ì²˜ë¦¬ì‹œê°„: {crawling_result.processing_time:.2f}ì´ˆ")
            if crawling_result.extracted_data:
                phone = crawling_result.extracted_data.get("phone", "ì—†ìŒ")
                email = crawling_result.extracted_data.get("email", "ì—†ìŒ")
                logger.info(f"      ì „í™”ë²ˆí˜¸: {phone}")
                logger.info(f"      ì´ë©”ì¼: {email}")
        
        # ì •ë¦¬
        await performance_monitor.stop_monitoring()
        await crawling_integration.cleanup()
        
        logger.info("âœ… ê¸°ë³¸ ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì™„ë£Œ")
    
    async def example_advanced_crawling_strategies(self):
        """ì˜ˆì œ 2: ê³ ê¸‰ í¬ë¡¤ë§ ì „ëµ"""
        logger.info("ğŸ¯ ê³ ê¸‰ í¬ë¡¤ë§ ì „ëµ í…ŒìŠ¤íŠ¸")
        
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ê°„ì†Œí™”)
        agent_system = AIAgentSystem()
        performance_monitor = PerformanceMonitor()
        await performance_monitor.start_monitoring()
        
        monitoring_integration = MonitoringIntegration(
            agent_system=agent_system,
            performance_monitor=performance_monitor
        )
        
        crawling_integration = CrawlingAgentIntegration(
            agent_system=agent_system,
            performance_monitor=performance_monitor,
            monitoring_integration=monitoring_integration
        )
        
        # ë‹¤ì–‘í•œ ì „ëµìœ¼ë¡œ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸
        strategies = [
            CrawlingStrategy.CONSERVATIVE,
            CrawlingStrategy.BALANCED,
            CrawlingStrategy.AGGRESSIVE,
            CrawlingStrategy.AI_ADAPTIVE
        ]
        
        test_targets = [
            create_crawling_target(
                name="í…ŒìŠ¤íŠ¸êµíšŒ",
                category="êµíšŒ",
                search_keywords=["í…ŒìŠ¤íŠ¸êµíšŒ", "êµíšŒ"]
            )
        ]
        
        strategy_results = {}
        
        for strategy in strategies:
            logger.info(f"ğŸ”¬ {strategy.value} ì „ëµ í…ŒìŠ¤íŠ¸")
            
            job_config = create_crawling_job_config(
                targets=test_targets,
                strategy=strategy,
                max_concurrent=2 if strategy == CrawlingStrategy.CONSERVATIVE else 5,
                enable_ai_validation=strategy in [CrawlingStrategy.BALANCED, CrawlingStrategy.AI_ADAPTIVE],
                enable_enrichment=strategy != CrawlingStrategy.CONSERVATIVE,
                enable_optimization=strategy == CrawlingStrategy.AGGRESSIVE
            )
            
            start_time = time.time()
            result = await crawling_integration.start_intelligent_crawling_job(job_config)
            end_time = time.time()
            
            strategy_results[strategy.value] = {
                "success_rate": result.success_rate,
                "processing_time": end_time - start_time,
                "confidence_score": sum(r.confidence_score for r in result.results) / len(result.results) if result.results else 0
            }
            
            logger.info(f"  ì„±ê³µë¥ : {result.success_rate:.1%}")
            logger.info(f"  ì²˜ë¦¬ì‹œê°„: {end_time - start_time:.1f}ì´ˆ")
            logger.info(f"  í‰ê·  ì‹ ë¢°ë„: {strategy_results[strategy.value]['confidence_score']:.3f}")
        
        # ì „ëµ ë¹„êµ ê²°ê³¼
        logger.info("ğŸ“Š ì „ëµë³„ ì„±ëŠ¥ ë¹„êµ:")
        for strategy, metrics in strategy_results.items():
            logger.info(f"  {strategy}: ì„±ê³µë¥  {metrics['success_rate']:.1%}, "
                       f"ì‹œê°„ {metrics['processing_time']:.1f}ì´ˆ, "
                       f"ì‹ ë¢°ë„ {metrics['confidence_score']:.3f}")
        
        # ì •ë¦¬
        await performance_monitor.stop_monitoring()
        await crawling_integration.cleanup()
        
        logger.info("âœ… ê³ ê¸‰ í¬ë¡¤ë§ ì „ëµ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    async def example_bulk_crawling_management(self):
        """ì˜ˆì œ 3: ëŒ€ëŸ‰ í¬ë¡¤ë§ ê´€ë¦¬"""
        logger.info("ğŸ¯ ëŒ€ëŸ‰ í¬ë¡¤ë§ ê´€ë¦¬ ì‹œì‘")
        
        # ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        agent_system = AIAgentSystem()
        performance_monitor = PerformanceMonitor()
        await performance_monitor.start_monitoring()
        
        monitoring_integration = MonitoringIntegration(
            agent_system=agent_system,
            performance_monitor=performance_monitor
        )
        
        service_config = IntelligentCrawlingConfig(
            max_concurrent_jobs=3,
            job_queue_size=20,
            enable_monitoring=True
        )
        
        crawling_service = await create_intelligent_crawling_service(
            agent_system=agent_system,
            performance_monitor=performance_monitor,
            monitoring_integration=monitoring_integration,
            config=service_config
        )
        
        # ëŒ€ëŸ‰ í¬ë¡¤ë§ ëŒ€ìƒ ìƒì„± (50ê°œ)
        bulk_targets = []
        for i in range(50):
            target_data = {
                "name": f"í…ŒìŠ¤íŠ¸ê¸°ê´€{i+1:02d}",
                "category": "êµíšŒ" if i % 3 == 0 else "í•™ì›" if i % 3 == 1 else "ì£¼ë¯¼ì„¼í„°",
                "search_keywords": [f"í…ŒìŠ¤íŠ¸ê¸°ê´€{i+1:02d}"]
            }
            bulk_targets.append(target_data)
        
        # 10ê°œì”© ë‚˜ëˆ„ì–´ 5ê°œ ì‘ì—…ìœ¼ë¡œ ì œì¶œ
        chunk_size = 10
        job_ids = []
        
        for i in range(0, len(bulk_targets), chunk_size):
            chunk = bulk_targets[i:i+chunk_size]
            
            job_id = await crawling_service.submit_crawling_job(
                targets=chunk,
                strategy=CrawlingStrategy.BALANCED,
                job_options={
                    "max_concurrent": 5,
                    "enable_ai_validation": True,
                    "enable_enrichment": True
                }
            )
            
            job_ids.append(job_id)
            logger.info(f"ğŸ“ ì‘ì—… ì œì¶œ: {job_id} ({len(chunk)}ê°œ ëŒ€ìƒ)")
        
        # ì‘ì—… ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§
        logger.info("ğŸ“Š ì‘ì—… ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        
        completed_jobs = 0
        while completed_jobs < len(job_ids):
            await asyncio.sleep(5)
            
            # ì„œë¹„ìŠ¤ ë©”íŠ¸ë¦­ ì¡°íšŒ
            metrics = crawling_service.get_service_metrics()
            active_jobs = crawling_service.get_active_jobs()
            
            logger.info(f"  í™œì„± ì‘ì—…: {len(active_jobs)}ê°œ")
            logger.info(f"  í í¬ê¸°: {metrics['current_queue_size']}")
            logger.info(f"  ì™„ë£Œëœ ì‘ì—…: {metrics['successful_jobs'] + metrics['failed_jobs']}/{len(job_ids)}")
            
            # ì™„ë£Œëœ ì‘ì—… ìˆ˜ í™•ì¸
            completed_jobs = metrics['successful_jobs'] + metrics['failed_jobs']
        
        # ìµœì¢… ê²°ê³¼ í™•ì¸
        final_metrics = crawling_service.get_service_metrics()
        recent_jobs = crawling_service.get_recent_jobs(len(job_ids))
        
        logger.info("ğŸ“Š ëŒ€ëŸ‰ í¬ë¡¤ë§ ìµœì¢… ê²°ê³¼:")
        logger.info(f"  ì´ ì‘ì—…: {len(job_ids)}ê°œ")
        logger.info(f"  ì„±ê³µí•œ ì‘ì—…: {final_metrics['successful_jobs']}ê°œ")
        logger.info(f"  ì‹¤íŒ¨í•œ ì‘ì—…: {final_metrics['failed_jobs']}ê°œ")
        logger.info(f"  ì´ ì²˜ë¦¬ ëŒ€ìƒ: {final_metrics['total_targets_processed']}ê°œ")
        logger.info(f"  í‰ê·  ì‘ì—… ì‹œê°„: {final_metrics['average_job_duration']:.1f}ì´ˆ")
        
        # ê°œë³„ ì‘ì—… ê²°ê³¼ ìš”ì•½
        total_success_rate = 0
        for job in recent_jobs:
            if job.get('success_rate') is not None:
                total_success_rate += job['success_rate']
        
        average_success_rate = total_success_rate / len(recent_jobs) if recent_jobs else 0
        logger.info(f"  í‰ê·  ì„±ê³µë¥ : {average_success_rate:.1%}")
        
        # ì„œë¹„ìŠ¤ ì¤‘ì§€
        await crawling_service.stop_service()
        await performance_monitor.stop_monitoring()
        
        logger.info("âœ… ëŒ€ëŸ‰ í¬ë¡¤ë§ ê´€ë¦¬ ì™„ë£Œ")
    
    async def example_scheduled_crawling(self):
        """ì˜ˆì œ 4: ìŠ¤ì¼€ì¤„ëœ í¬ë¡¤ë§"""
        logger.info("ğŸ¯ ìŠ¤ì¼€ì¤„ëœ í¬ë¡¤ë§ ì‹œì‘")
        
        # ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        agent_system = AIAgentSystem()
        performance_monitor = PerformanceMonitor()
        await performance_monitor.start_monitoring()
        
        monitoring_integration = MonitoringIntegration(
            agent_system=agent_system,
            performance_monitor=performance_monitor
        )
        
        service_config = IntelligentCrawlingConfig(
            enable_scheduling=True,
            enable_monitoring=True
        )
        
        crawling_service = await create_intelligent_crawling_service(
            agent_system=agent_system,
            performance_monitor=performance_monitor,
            monitoring_integration=monitoring_integration,
            config=service_config
        )
        
        # ìŠ¤ì¼€ì¤„ëœ í¬ë¡¤ë§ ì‘ì—… ë“±ë¡
        scheduled_targets = [
            {
                "name": "ì¼ì¼ì ê²€ëŒ€ìƒ",
                "category": "êµíšŒ",
                "search_keywords": ["ì¼ì¼ì ê²€ëŒ€ìƒ", "êµíšŒ"]
            }
        ]
        
        # ë§¤ì‹œê°„ ì‹¤í–‰ë˜ëŠ” ìŠ¤ì¼€ì¤„ ë“±ë¡
        schedule_success = await crawling_service.schedule_crawling_job(
            schedule_id="hourly_crawling",
            targets=scheduled_targets,
            cron_expression="0 * * * *",  # ë§¤ì‹œê°„ 0ë¶„ì— ì‹¤í–‰
            strategy=CrawlingStrategy.CONSERVATIVE,
            job_options={
                "max_concurrent": 2,
                "enable_ai_validation": False,
                "enable_enrichment": False
            },
            enabled=True
        )
        
        logger.info(f"ğŸ“… ìŠ¤ì¼€ì¤„ ë“±ë¡ ê²°ê³¼: {schedule_success}")
        
        # ì¦‰ì‹œ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ìš© ìŠ¤ì¼€ì¤„ ë“±ë¡
        test_targets = [
            {
                "name": "í…ŒìŠ¤íŠ¸ì¦‰ì‹œì‹¤í–‰",
                "category": "í•™ì›",
                "search_keywords": ["í…ŒìŠ¤íŠ¸ì¦‰ì‹œì‹¤í–‰", "í•™ì›"]
            }
        ]
        
        # 1ë¶„ í›„ ì‹¤í–‰ë˜ëŠ” í…ŒìŠ¤íŠ¸ ìŠ¤ì¼€ì¤„
        test_schedule_success = await crawling_service.schedule_crawling_job(
            schedule_id="test_immediate",
            targets=test_targets,
            cron_expression="* * * * *",  # ë§¤ë¶„ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©)
            strategy=CrawlingStrategy.BALANCED,
            enabled=True
        )
        
        logger.info(f"ğŸ“… í…ŒìŠ¤íŠ¸ ìŠ¤ì¼€ì¤„ ë“±ë¡: {test_schedule_success}")
        
        # ìŠ¤ì¼€ì¤„ëœ ì‘ì—… ëª©ë¡ í™•ì¸
        scheduled_jobs = crawling_service.get_scheduled_jobs()
        logger.info(f"ğŸ“‹ ë“±ë¡ëœ ìŠ¤ì¼€ì¤„: {len(scheduled_jobs)}ê°œ")
        
        for schedule in scheduled_jobs:
            logger.info(f"  - {schedule['schedule_id']}: {schedule['cron_expression']} "
                       f"(í™œì„±í™”: {schedule['enabled']})")
        
        # ìŠ¤ì¼€ì¤„ ì‹¤í–‰ ëª¨ë‹ˆí„°ë§ (2ë¶„ê°„)
        logger.info("ğŸ“Š ìŠ¤ì¼€ì¤„ ì‹¤í–‰ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (2ë¶„ê°„)")
        
        for i in range(12):  # 10ì´ˆì”© 12ë²ˆ = 2ë¶„
            await asyncio.sleep(10)
            
            metrics = crawling_service.get_service_metrics()
            active_jobs = crawling_service.get_active_jobs()
            
            logger.info(f"  [{i+1}/12] í™œì„± ì‘ì—…: {len(active_jobs)}ê°œ, "
                       f"ì™„ë£Œ: {metrics['successful_jobs'] + metrics['failed_jobs']}ê°œ")
            
            # ìŠ¤ì¼€ì¤„ ì‹¤í–‰ ê²°ê³¼ í™•ì¸
            recent_jobs = crawling_service.get_recent_jobs(5)
            scheduled_executions = [job for job in recent_jobs 
                                  if job['job_id'].startswith('scheduled_')]
            
            if scheduled_executions:
                logger.info(f"  ìŠ¤ì¼€ì¤„ëœ ì‘ì—… ì‹¤í–‰: {len(scheduled_executions)}ê°œ")
        
        # ìµœì¢… ê²°ê³¼
        final_scheduled_jobs = crawling_service.get_scheduled_jobs()
        final_recent_jobs = crawling_service.get_recent_jobs(10)
        
        scheduled_executions = [job for job in final_recent_jobs 
                              if job['job_id'].startswith('scheduled_')]
        
        logger.info("ğŸ“Š ìŠ¤ì¼€ì¤„ëœ í¬ë¡¤ë§ ìµœì¢… ê²°ê³¼:")
        logger.info(f"  ë“±ë¡ëœ ìŠ¤ì¼€ì¤„: {len(final_scheduled_jobs)}ê°œ")
        logger.info(f"  ì‹¤í–‰ëœ ìŠ¤ì¼€ì¤„ ì‘ì—…: {len(scheduled_executions)}ê°œ")
        
        for schedule in final_scheduled_jobs:
            logger.info(f"  - {schedule['schedule_id']}: ì‹¤í–‰íšŸìˆ˜ {schedule['run_count']}íšŒ")
        
        # ì„œë¹„ìŠ¤ ì¤‘ì§€
        await crawling_service.stop_service()
        await performance_monitor.stop_monitoring()
        
        logger.info("âœ… ìŠ¤ì¼€ì¤„ëœ í¬ë¡¤ë§ ì™„ë£Œ")
    
    async def example_performance_optimization_integration(self):
        """ì˜ˆì œ 5: ì„±ëŠ¥ ìµœì í™” í†µí•©"""
        logger.info("ğŸ¯ ì„±ëŠ¥ ìµœì í™” í†µí•© ì‹œì‘")
        
        # GCP ìµœì í™”ë¥¼ í¬í•¨í•œ ì „ì²´ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        agent_system = AIAgentSystem()
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„° ì´ˆê¸°í™”
        performance_monitor = PerformanceMonitor()
        await performance_monitor.start_monitoring()
        
        # GCP ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        gcp_optimizer = GCPOptimizer()
        optimization_service_config = OptimizationServiceConfig(
            auto_optimization_mode=True,
            optimization_interval=300,  # 5ë¶„ë§ˆë‹¤
            analysis_interval=180       # 3ë¶„ë§ˆë‹¤
        )
        
        monitoring_integration = MonitoringIntegration(
            agent_system=agent_system,
            performance_monitor=performance_monitor
        )
        
        gcp_optimization_service = GCPOptimizationService(
            gcp_optimizer=gcp_optimizer,
            performance_monitor=performance_monitor,
            monitoring_integration=monitoring_integration,
            agent_system=agent_system,
            config=optimization_service_config
        )
        
        # í¬ë¡¤ë§ í†µí•© ì‹œìŠ¤í…œ (ìµœì í™” í¬í•¨)
        crawling_integration = CrawlingAgentIntegration(
            agent_system=agent_system,
            performance_monitor=performance_monitor,
            monitoring_integration=monitoring_integration,
            gcp_optimization_service=gcp_optimization_service
        )
        
        # GCP ìµœì í™” ì„œë¹„ìŠ¤ ì‹œì‘
        await gcp_optimization_service.start_service()
        
        # ì„±ëŠ¥ ë¶€í•˜ë¥¼ ì£¼ëŠ” í¬ë¡¤ë§ ì‘ì—… ì‹¤í–‰
        performance_test_targets = []
        for i in range(20):
            target = create_crawling_target(
                name=f"ì„±ëŠ¥í…ŒìŠ¤íŠ¸ëŒ€ìƒ{i+1:02d}",
                category="êµíšŒ",
                search_keywords=[f"ì„±ëŠ¥í…ŒìŠ¤íŠ¸ëŒ€ìƒ{i+1:02d}", "êµíšŒ"],
                priority=1 if i < 10 else 2
            )
            performance_test_targets.append(target)
        
        # ì ê·¹ì  ì „ëµìœ¼ë¡œ ì„±ëŠ¥ ë¶€í•˜ ìƒì„±
        job_config = create_crawling_job_config(
            targets=performance_test_targets,
            strategy=CrawlingStrategy.AGGRESSIVE,
            max_concurrent=8,
            enable_ai_validation=True,
            enable_enrichment=True,
            enable_optimization=True  # ìµœì í™” í™œì„±í™”
        )
        
        logger.info("ğŸš€ ì„±ëŠ¥ ìµœì í™”ê°€ í¬í•¨ëœ í¬ë¡¤ë§ ì‹œì‘")
        
        # í¬ë¡¤ë§ ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ)
        crawling_task = asyncio.create_task(
            crawling_integration.start_intelligent_crawling_job(job_config)
        )
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™” ì¶”ì  (1ë¶„ê°„)
        logger.info("ğŸ“Š ì„±ëŠ¥ ìµœì í™” ì¶”ì  ì‹œì‘")
        
        for i in range(6):  # 10ì´ˆì”© 6ë²ˆ = 1ë¶„
            await asyncio.sleep(10)
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ
            current_metrics = performance_monitor.get_current_metrics()
            system_snapshot = performance_monitor.get_system_snapshot()
            
            logger.info(f"  [{i+1}/6] ì‹œìŠ¤í…œ ìƒíƒœ:")
            logger.info(f"    CPU: {system_snapshot.cpu_percent:.1f}%")
            logger.info(f"    ë©”ëª¨ë¦¬: {system_snapshot.memory_percent:.1f}%")
            logger.info(f"    ìˆ˜ì§‘ëœ ë©”íŠ¸ë¦­: {len(current_metrics)}ê°œ")
            
            # GCP ìµœì í™” ì„œë¹„ìŠ¤ ìƒíƒœ
            optimization_metrics = gcp_optimization_service.get_service_metrics()
            logger.info(f"    ìµœì í™” ì‘ì—…: {optimization_metrics.total_optimizations}ê°œ")
            logger.info(f"    ê°œì„  ì‚¬í•­: {optimization_metrics.successful_optimizations}ê°œ")
        
        # í¬ë¡¤ë§ ì™„ë£Œ ëŒ€ê¸°
        crawling_result = await crawling_task
        
        # ìµœì¢… ì„±ëŠ¥ ìµœì í™” ê²°ê³¼
        final_optimization_metrics = gcp_optimization_service.get_service_metrics()
        final_performance_report = performance_monitor.generate_performance_report(
            time_range_minutes=5
        )
        
        logger.info("ğŸ“Š ì„±ëŠ¥ ìµœì í™” í†µí•© ìµœì¢… ê²°ê³¼:")
        logger.info(f"  í¬ë¡¤ë§ ì„±ê³µë¥ : {crawling_result.success_rate:.1%}")
        logger.info(f"  í¬ë¡¤ë§ ì²˜ë¦¬ ì‹œê°„: {crawling_result.total_processing_time:.1f}ì´ˆ")
        logger.info(f"  ìµœì í™” ì‘ì—… ìˆ˜í–‰: {final_optimization_metrics.total_optimizations}ê°œ")
        logger.info(f"  ì„±ê³µí•œ ìµœì í™”: {final_optimization_metrics.successful_optimizations}ê°œ")
        
        if final_performance_report:
            logger.info(f"  í‰ê·  CPU ì‚¬ìš©ë¥ : {final_performance_report.get('avg_cpu', 0):.1f}%")
            logger.info(f"  í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {final_performance_report.get('avg_memory', 0):.1f}%")
            logger.info(f"  ì„±ëŠ¥ ì•Œë¦¼ ìˆ˜: {final_performance_report.get('alert_count', 0)}ê°œ")
        
        # ì„œë¹„ìŠ¤ ì¤‘ì§€
        await gcp_optimization_service.stop_service()
        await performance_monitor.stop_monitoring()
        await crawling_integration.cleanup()
        
        logger.info("âœ… ì„±ëŠ¥ ìµœì í™” í†µí•© ì™„ë£Œ")
    
    async def example_real_time_monitoring(self):
        """ì˜ˆì œ 6: ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§"""
        logger.info("ğŸ¯ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        
        # ì „ì²´ í†µí•© ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        agent_system = AIAgentSystem()
        performance_monitor = PerformanceMonitor()
        await performance_monitor.start_monitoring()
        
        monitoring_config = MonitoringConfig(
            enable_agent_monitoring=True,
            enable_system_monitoring=True,
            enable_performance_tracking=True,
            agent_monitoring_interval=15,
            system_monitoring_interval=30,
            performance_tracking_interval=10
        )
        
        monitoring_integration = MonitoringIntegration(
            agent_system=agent_system,
            performance_monitor=performance_monitor,
            config=monitoring_config
        )
        
        # í†µí•© ëª¨ë‹ˆí„°ë§ ì‹œì‘
        await monitoring_integration.start_integrated_monitoring()
        
        # ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì„œë¹„ìŠ¤ (ëª¨ë‹ˆí„°ë§ í™œì„±í™”)
        service_config = IntelligentCrawlingConfig(
            enable_monitoring=True,
            max_concurrent_jobs=2
        )
        
        crawling_service = await create_intelligent_crawling_service(
            agent_system=agent_system,
            performance_monitor=performance_monitor,
            monitoring_integration=monitoring_integration,
            config=service_config
        )
        
        # ëª¨ë‹ˆí„°ë§ ëŒ€ìƒ í¬ë¡¤ë§ ì‘ì—…ë“¤ ì œì¶œ
        monitoring_targets = [
            {
                "name": f"ëª¨ë‹ˆí„°ë§ëŒ€ìƒ{i+1}",
                "category": "êµíšŒ" if i % 2 == 0 else "í•™ì›",
                "search_keywords": [f"ëª¨ë‹ˆí„°ë§ëŒ€ìƒ{i+1}"]
            }
            for i in range(10)
        ]
        
        # ì—¬ëŸ¬ ì‘ì—…ì„ ìˆœì°¨ì ìœ¼ë¡œ ì œì¶œí•˜ì—¬ ëª¨ë‹ˆí„°ë§ ë°ì´í„° ìƒì„±
        job_ids = []
        for i in range(3):
            job_id = await crawling_service.submit_crawling_job(
                targets=monitoring_targets[i*3:(i+1)*3],
                strategy=CrawlingStrategy.BALANCED
            )
            job_ids.append(job_id)
            logger.info(f"ğŸ“ ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸ ì‘ì—… ì œì¶œ: {job_id}")
        
        # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë°ì´í„° ìˆ˜ì§‘ ë° ì¶œë ¥ (2ë¶„ê°„)
        logger.info("ğŸ“Š ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (2ë¶„ê°„)")
        
        monitoring_data = []
        
        for i in range(12):  # 10ì´ˆì”© 12ë²ˆ = 2ë¶„
            await asyncio.sleep(10)
            
            # í†µí•© ëª¨ë‹ˆí„°ë§ ìƒíƒœ ì¡°íšŒ
            monitoring_status = monitoring_integration.get_monitoring_status()
            service_metrics = crawling_service.get_service_metrics()
            system_snapshot = performance_monitor.get_system_snapshot()
            
            # ëª¨ë‹ˆí„°ë§ ë°ì´í„° ê¸°ë¡
            timestamp = datetime.now()
            data_point = {
                "timestamp": timestamp.isoformat(),
                "system": {
                    "cpu_percent": system_snapshot.cpu_percent,
                    "memory_percent": system_snapshot.memory_percent,
                    "disk_usage": system_snapshot.disk_usage_percent
                },
                "crawling_service": {
                    "active_jobs": service_metrics["active_jobs_count"],
                    "completed_jobs": service_metrics["completed_jobs_count"],
                    "queue_size": service_metrics["current_queue_size"],
                    "success_rate": service_metrics.get("successful_jobs", 0) / max(service_metrics.get("total_jobs_processed", 1), 1)
                },
                "monitoring": {
                    "agents_monitored": monitoring_status.get("active_agents", 0),
                    "performance_alerts": monitoring_status.get("recent_alerts", 0),
                    "monitoring_uptime": monitoring_status.get("uptime_seconds", 0)
                }
            }
            
            monitoring_data.append(data_point)
            
            # ì‹¤ì‹œê°„ ì¶œë ¥
            logger.info(f"  [{i+1:2d}/12] {timestamp.strftime('%H:%M:%S')}")
            logger.info(f"    ì‹œìŠ¤í…œ: CPU {data_point['system']['cpu_percent']:.1f}%, "
                       f"ë©”ëª¨ë¦¬ {data_point['system']['memory_percent']:.1f}%")
            logger.info(f"    í¬ë¡¤ë§: í™œì„± {data_point['crawling_service']['active_jobs']}ê°œ, "
                       f"ì™„ë£Œ {data_point['crawling_service']['completed_jobs']}ê°œ")
            logger.info(f"    ëª¨ë‹ˆí„°ë§: ì—ì´ì „íŠ¸ {data_point['monitoring']['agents_monitored']}ê°œ ì¶”ì  ì¤‘")
        
        # ëª¨ë‹ˆí„°ë§ ë°ì´í„° ë¶„ì„
        logger.info("ğŸ“ˆ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë°ì´í„° ë¶„ì„:")
        
        # CPU/ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  í†µê³„
        cpu_values = [d['system']['cpu_percent'] for d in monitoring_data]
        memory_values = [d['system']['memory_percent'] for d in monitoring_data]
        
        logger.info(f"  CPU ì‚¬ìš©ë¥ : í‰ê·  {sum(cpu_values)/len(cpu_values):.1f}%, "
                   f"ìµœëŒ€ {max(cpu_values):.1f}%, ìµœì†Œ {min(cpu_values):.1f}%")
        logger.info(f"  ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : í‰ê·  {sum(memory_values)/len(memory_values):.1f}%, "
                   f"ìµœëŒ€ {max(memory_values):.1f}%, ìµœì†Œ {min(memory_values):.1f}%")
        
        # í¬ë¡¤ë§ ì„œë¹„ìŠ¤ í™œë™ í†µê³„
        max_active_jobs = max(d['crawling_service']['active_jobs'] for d in monitoring_data)
        total_completed = max(d['crawling_service']['completed_jobs'] for d in monitoring_data)
        
        logger.info(f"  ìµœëŒ€ ë™ì‹œ ì‘ì—…: {max_active_jobs}ê°œ")
        logger.info(f"  ì´ ì™„ë£Œ ì‘ì—…: {total_completed}ê°œ")
        
        # í†µí•© ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
        performance_report = monitoring_integration.generate_integrated_report(
            time_range_minutes=2
        )
        
        if performance_report:
            logger.info("ğŸ“‹ í†µí•© ì„±ëŠ¥ ë¦¬í¬íŠ¸:")
            logger.info(f"  ëª¨ë‹ˆí„°ë§ ê¸°ê°„: {performance_report.get('monitoring_duration', 0):.0f}ì´ˆ")
            logger.info(f"  ìˆ˜ì§‘ëœ ë©”íŠ¸ë¦­: {performance_report.get('total_metrics', 0)}ê°œ")
            logger.info(f"  ë°œìƒí•œ ì•Œë¦¼: {performance_report.get('total_alerts', 0)}ê°œ")
            logger.info(f"  ì‹œìŠ¤í…œ ì•ˆì •ì„±: {performance_report.get('system_stability', 0):.1%}")
        
        # ì„œë¹„ìŠ¤ ì¤‘ì§€
        await crawling_service.stop_service()
        await monitoring_integration.stop_integrated_monitoring()
        await performance_monitor.stop_monitoring()
        
        logger.info("âœ… ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì™„ë£Œ")
    
    async def example_comprehensive_demo(self):
        """ì˜ˆì œ 7: ì¢…í•© ë°ëª¨"""
        logger.info("ğŸ¯ í¬ë¡¤ë§ ì‹œìŠ¤í…œ í†µí•© ì¢…í•© ë°ëª¨ ì‹œì‘")
        
        # ì „ì²´ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ëª¨ë“  êµ¬ì„± ìš”ì†Œ í¬í•¨)
        logger.info("ğŸ”§ ì „ì²´ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        
        # 1. AI ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ
        system_config = SystemConfig(
            name="ComprehensiveDemo",
            version="1.0.0",
            max_concurrent_agents=15
        )
        agent_system = AIAgentSystem(system_config)
        
        # 2. ì„±ëŠ¥ ëª¨ë‹ˆí„°
        performance_monitor = PerformanceMonitor()
        await performance_monitor.start_monitoring()
        
        # 3. í†µí•© ëª¨ë‹ˆí„°ë§
        monitoring_config = MonitoringConfig(
            enable_agent_monitoring=True,
            enable_system_monitoring=True,
            enable_performance_tracking=True
        )
        monitoring_integration = MonitoringIntegration(
            agent_system=agent_system,
            performance_monitor=performance_monitor,
            config=monitoring_config
        )
        await monitoring_integration.start_integrated_monitoring()
        
        # 4. GCP ìµœì í™” (ì„ íƒì )
        gcp_optimizer = GCPOptimizer()
        gcp_optimization_service = GCPOptimizationService(
            gcp_optimizer=gcp_optimizer,
            performance_monitor=performance_monitor,
            monitoring_integration=monitoring_integration,
            agent_system=agent_system
        )
        await gcp_optimization_service.start_service()
        
        # 5. ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì„œë¹„ìŠ¤
        service_config = IntelligentCrawlingConfig(
            max_concurrent_jobs=4,
            enable_scheduling=True,
            enable_monitoring=True,
            enable_optimization=True
        )
        
        crawling_service = await create_intelligent_crawling_service(
            agent_system=agent_system,
            performance_monitor=performance_monitor,
            monitoring_integration=monitoring_integration,
            gcp_optimization_service=gcp_optimization_service,
            config=service_config
        )
        
        logger.info("âœ… ì „ì²´ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ì¢…í•© ë°ëª¨ ì‹œë‚˜ë¦¬ì˜¤
        demo_scenarios = [
            ("ì¦‰ì‹œ í¬ë¡¤ë§ ì‘ì—…", self._demo_immediate_crawling),
            ("ìŠ¤ì¼€ì¤„ëœ í¬ë¡¤ë§ ì„¤ì •", self._demo_scheduled_setup),
            ("ëŒ€ëŸ‰ í¬ë¡¤ë§ ì²˜ë¦¬", self._demo_bulk_processing),
            ("ì„±ëŠ¥ ìµœì í™” í…ŒìŠ¤íŠ¸", self._demo_optimization_test),
            ("ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§", self._demo_real_time_tracking)
        ]
        
        demo_results = {}
        
        for scenario_name, scenario_func in demo_scenarios:
            logger.info(f"\nğŸ¬ ì‹œë‚˜ë¦¬ì˜¤: {scenario_name}")
            
            try:
                result = await scenario_func(crawling_service, monitoring_integration, gcp_optimization_service)
                demo_results[scenario_name] = {"success": True, "result": result}
                logger.info(f"âœ… {scenario_name} ì™„ë£Œ")
            except Exception as e:
                demo_results[scenario_name] = {"success": False, "error": str(e)}
                logger.error(f"âŒ {scenario_name} ì‹¤íŒ¨: {e}")
            
            await asyncio.sleep(2)
        
        # ì¢…í•© ê²°ê³¼ ë¦¬í¬íŠ¸
        logger.info("\nğŸ“Š í¬ë¡¤ë§ ì‹œìŠ¤í…œ í†µí•© ì¢…í•© ë°ëª¨ ê²°ê³¼:")
        
        successful_scenarios = len([r for r in demo_results.values() if r["success"]])
        total_scenarios = len(demo_scenarios)
        
        logger.info(f"  ì‹¤í–‰ëœ ì‹œë‚˜ë¦¬ì˜¤: {total_scenarios}ê°œ")
        logger.info(f"  ì„±ê³µí•œ ì‹œë‚˜ë¦¬ì˜¤: {successful_scenarios}ê°œ")
        logger.info(f"  ì„±ê³µë¥ : {successful_scenarios/total_scenarios:.1%}")
        
        # ì‹œìŠ¤í…œ ì „ì²´ ë©”íŠ¸ë¦­
        final_service_metrics = crawling_service.get_service_metrics()
        final_monitoring_status = monitoring_integration.get_monitoring_status()
        final_optimization_metrics = gcp_optimization_service.get_service_metrics()
        
        logger.info(f"  ì´ ì²˜ë¦¬ëœ í¬ë¡¤ë§ ì‘ì—…: {final_service_metrics['total_jobs_processed']}ê°œ")
        logger.info(f"  ì „ì²´ ì²˜ë¦¬ ëŒ€ìƒ: {final_service_metrics['total_targets_processed']}ê°œ")
        logger.info(f"  í‰ê·  ì‘ì—… ì‹œê°„: {final_service_metrics['average_job_duration']:.1f}ì´ˆ")
        logger.info(f"  ìµœì í™” ê°œì„  ì‚¬í•­: {final_optimization_metrics.successful_optimizations}ê°œ")
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ ìƒì„¸ ê²°ê³¼
        for scenario_name, result in demo_results.items():
            if result["success"]:
                logger.info(f"  âœ… {scenario_name}: ì„±ê³µ")
            else:
                logger.info(f"  âŒ {scenario_name}: ì‹¤íŒ¨ - {result['error']}")
        
        # í†µí•© ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
        comprehensive_report = monitoring_integration.generate_integrated_report(
            time_range_minutes=10
        )
        
        if comprehensive_report:
            logger.info("ğŸ“‹ ì¢…í•© ì„±ëŠ¥ ë¦¬í¬íŠ¸:")
            logger.info(f"  ì „ì²´ ëª¨ë‹ˆí„°ë§ ì‹œê°„: {comprehensive_report.get('monitoring_duration', 0):.0f}ì´ˆ")
            logger.info(f"  ìˆ˜ì§‘ëœ ì´ ë©”íŠ¸ë¦­: {comprehensive_report.get('total_metrics', 0)}ê°œ")
            logger.info(f"  ì‹œìŠ¤í…œ ì•ˆì •ì„±: {comprehensive_report.get('system_stability', 0):.1%}")
            logger.info(f"  AI ì—ì´ì „íŠ¸ íš¨ìœ¨ì„±: {comprehensive_report.get('agent_efficiency', 0):.1%}")
        
        # ì „ì²´ ì‹œìŠ¤í…œ ì •ë¦¬
        logger.info("ğŸ§¹ ì „ì²´ ì‹œìŠ¤í…œ ì •ë¦¬ ì¤‘...")
        
        await crawling_service.stop_service()
        await gcp_optimization_service.stop_service()
        await monitoring_integration.stop_integrated_monitoring()
        await performance_monitor.stop_monitoring()
        
        logger.info("âœ… í¬ë¡¤ë§ ì‹œìŠ¤í…œ í†µí•© ì¢…í•© ë°ëª¨ ì™„ë£Œ")
        
        return demo_results
    
    async def _demo_immediate_crawling(self, crawling_service, monitoring_integration, gcp_optimization_service):
        """ì¦‰ì‹œ í¬ë¡¤ë§ ì‘ì—… ë°ëª¨"""
        targets = [
            {"name": "ë°ëª¨êµíšŒ1", "category": "êµíšŒ", "search_keywords": ["ë°ëª¨êµíšŒ1"]},
            {"name": "ë°ëª¨í•™ì›1", "category": "í•™ì›", "search_keywords": ["ë°ëª¨í•™ì›1"]},
            {"name": "ë°ëª¨êµ¬ì²­1", "category": "ì£¼ë¯¼ì„¼í„°", "search_keywords": ["ë°ëª¨êµ¬ì²­1"]}
        ]
        
        job_id = await crawling_service.submit_crawling_job(
            targets=targets,
            strategy=CrawlingStrategy.BALANCED
        )
        
        # ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
        for _ in range(30):
            await asyncio.sleep(2)
            status = crawling_service.get_job_status(job_id)
            if status and status.get("status") in ["completed", "failed"]:
                break
        
        final_status = crawling_service.get_job_status(job_id)
        return {
            "job_id": job_id,
            "status": final_status.get("status") if final_status else "timeout",
            "targets_processed": len(targets)
        }
    
    async def _demo_scheduled_setup(self, crawling_service, monitoring_integration, gcp_optimization_service):
        """ìŠ¤ì¼€ì¤„ëœ í¬ë¡¤ë§ ì„¤ì • ë°ëª¨"""
        schedule_targets = [
            {"name": "ìŠ¤ì¼€ì¤„ë°ëª¨", "category": "êµíšŒ", "search_keywords": ["ìŠ¤ì¼€ì¤„ë°ëª¨"]}
        ]
        
        success = await crawling_service.schedule_crawling_job(
            schedule_id="demo_schedule",
            targets=schedule_targets,
            cron_expression="*/1 * * * *",  # ë§¤ë¶„ ì‹¤í–‰ (ë°ëª¨ìš©)
            strategy=CrawlingStrategy.CONSERVATIVE
        )
        
        # ìŠ¤ì¼€ì¤„ í™•ì¸
        scheduled_jobs = crawling_service.get_scheduled_jobs()
        
        return {
            "schedule_created": success,
            "total_schedules": len(scheduled_jobs),
            "schedule_id": "demo_schedule"
        }
    
    async def _demo_bulk_processing(self, crawling_service, monitoring_integration, gcp_optimization_service):
        """ëŒ€ëŸ‰ í¬ë¡¤ë§ ì²˜ë¦¬ ë°ëª¨"""
        bulk_targets = [
            {"name": f"ë²Œí¬{i:02d}", "category": "êµíšŒ", "search_keywords": [f"ë²Œí¬{i:02d}"]}
            for i in range(15)
        ]
        
        job_id = await crawling_service.submit_crawling_job(
            targets=bulk_targets,
            strategy=CrawlingStrategy.AGGRESSIVE,
            job_options={"max_concurrent": 8}
        )
        
        # ì²˜ë¦¬ ìƒí™© ëª¨ë‹ˆí„°ë§
        for _ in range(45):
            await asyncio.sleep(2)
            status = crawling_service.get_job_status(job_id)
            if status and status.get("status") in ["completed", "failed"]:
                break
        
        final_status = crawling_service.get_job_status(job_id)
        
        return {
            "job_id": job_id,
            "targets_count": len(bulk_targets),
            "status": final_status.get("status") if final_status else "timeout",
            "success_rate": final_status.get("success_rate", 0) if final_status else 0
        }
    
    async def _demo_optimization_test(self, crawling_service, monitoring_integration, gcp_optimization_service):
        """ì„±ëŠ¥ ìµœì í™” í…ŒìŠ¤íŠ¸ ë°ëª¨"""
        # ìµœì í™” íŠ¸ë¦¬ê±°
        await gcp_optimization_service.trigger_optimization_analysis()
        
        # ìµœì í™” ìƒíƒœ í™•ì¸
        await asyncio.sleep(5)
        
        optimization_metrics = gcp_optimization_service.get_service_metrics()
        
        return {
            "optimization_triggered": True,
            "total_optimizations": optimization_metrics.total_optimizations,
            "successful_optimizations": optimization_metrics.successful_optimizations
        }
    
    async def _demo_real_time_tracking(self, crawling_service, monitoring_integration, gcp_optimization_service):
        """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë°ëª¨"""
        # ëª¨ë‹ˆí„°ë§ ìƒíƒœ ìˆ˜ì§‘
        tracking_data = []
        
        for i in range(6):  # 30ì´ˆê°„ 5ì´ˆì”© ìˆ˜ì§‘
            await asyncio.sleep(5)
            
            service_metrics = crawling_service.get_service_metrics()
            monitoring_status = monitoring_integration.get_monitoring_status()
            
            tracking_data.append({
                "timestamp": datetime.now().isoformat(),
                "active_jobs": service_metrics["active_jobs_count"],
                "completed_jobs": service_metrics["completed_jobs_count"],
                "monitoring_agents": monitoring_status.get("active_agents", 0)
            })
        
        return {
            "tracking_duration": 30,
            "data_points": len(tracking_data),
            "final_metrics": tracking_data[-1] if tracking_data else {}
        }


# ì‚¬ìš© ì˜ˆì œ ì‹¤í–‰ í•¨ìˆ˜
async def run_crawling_integration_examples():
    """í¬ë¡¤ë§ í†µí•© ì˜ˆì œ ì‹¤í–‰"""
    examples = CrawlingIntegrationExamples()
    await examples.run_all_examples()


if __name__ == "__main__":
    # ê°œë³„ ì˜ˆì œ ì‹¤í–‰
    asyncio.run(run_crawling_integration_examples()) 