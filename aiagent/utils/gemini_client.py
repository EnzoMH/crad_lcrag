"""
ğŸ¤– Langchain GoogleGenerativeAI ê¸°ë°˜ Gemini í´ë¼ì´ì–¸íŠ¸

ë©€í‹° API í‚¤ ì§€ì› ë° ë¡œë“œë°¸ëŸ°ì‹± ê¸°ëŠ¥
- ìë™ API í‚¤ ìˆœí™˜
- ìš”ì²­ ì œí•œ ê´€ë¦¬
- ì‹¤íŒ¨ ì‹œ ìë™ ì¬ì‹œë„
- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
"""

import os
import time
import asyncio
import random
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass
from collections import deque
from enum import Enum

from langchain_google_genai import GoogleGenerativeAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain.chains import LLMChain

from loguru import logger
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


class ModelType(Enum):
    """Gemini ëª¨ë¸ íƒ€ì…"""
    GEMINI_PRO = "gemini-pro"
    GEMINI_PRO_VISION = "gemini-pro-vision"
    GEMINI_1_5_FLASH = "gemini-1.5-flash"
    GEMINI_1_5_PRO = "gemini-1.5-pro"


@dataclass
class APIKeyStatus:
    """API í‚¤ ìƒíƒœ ì •ë³´"""
    key: str
    is_active: bool = True
    request_count: int = 0
    last_request_time: float = 0.0
    error_count: int = 0
    success_count: int = 0
    
    @property
    def success_rate(self) -> float:
        total = self.success_count + self.error_count
        return self.success_count / total if total > 0 else 0.0


@dataclass
class RequestMetrics:
    """ìš”ì²­ ë©”íŠ¸ë¦­ ì •ë³´"""
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    average_response_time: float = 0.0
    requests_per_minute: float = 0.0
    
    @property
    def success_rate(self) -> float:
        return self.successful_requests / self.total_requests if self.total_requests > 0 else 0.0


class GeminiClient:
    """
    ğŸ¤– Langchain ê¸°ë°˜ Gemini API í´ë¼ì´ì–¸íŠ¸
    
    Features:
    - ë©€í‹° API í‚¤ ì§€ì›
    - ìë™ ë¡œë“œë°¸ëŸ°ì‹±
    - ìš”ì²­ ì œí•œ ê´€ë¦¬
    - ì‹¤íŒ¨ ì‹œ ìë™ ì¬ì‹œë„
    - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    """
    
    def __init__(
        self,
        model_type: ModelType = ModelType.GEMINI_1_5_FLASH,
        temperature: float = 0.7,
        max_tokens: int = 1000,
        max_retries: int = 3,
        retry_delay: float = 1.0,
        rpm_limit: int = 2000,
        tpm_limit: int = 4000000
    ):
        """
        Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        
        Args:
            model_type: ì‚¬ìš©í•  Gemini ëª¨ë¸ íƒ€ì…
            temperature: ì‘ë‹µ ì°½ì˜ì„± (0.0-1.0)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
            retry_delay: ì¬ì‹œë„ ì§€ì—° ì‹œê°„ (ì´ˆ)
            rpm_limit: ë¶„ë‹¹ ìš”ì²­ ì œí•œ
            tpm_limit: ë¶„ë‹¹ í† í° ì œí•œ
        """
        self.model_type = model_type
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.rpm_limit = rpm_limit
        self.tpm_limit = tpm_limit
        
        # API í‚¤ ë¡œë“œ ë° ì´ˆê¸°í™”
        self.api_keys = self._load_api_keys()
        self.api_key_status = {
            key: APIKeyStatus(key=key) for key in self.api_keys
        }
        
        # LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        self.llm_instances = {}
        self._initialize_llm_instances()
        
        # ìš”ì²­ ì¶”ì 
        self.request_history = deque(maxlen=10000)
        self.metrics = RequestMetrics()
        
        # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ API í‚¤ ì¸ë±ìŠ¤
        self.current_key_index = 0
        
        logger.info(f"ğŸ¤– Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ - {len(self.api_keys)}ê°œ API í‚¤ ë¡œë“œ")
    
    def _load_api_keys(self) -> List[str]:
        """í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ë“¤ì„ ë¡œë“œ"""
        keys = []
        
        # ê¸°ë³¸ API í‚¤
        primary_key = os.getenv("GEMINI_API_KEY")
        if primary_key:
            keys.append(primary_key)
        
        # ì¶”ê°€ API í‚¤ë“¤
        for i in range(2, 10):  # GEMINI_API_KEY_2 ~ GEMINI_API_KEY_9
            key = os.getenv(f"GEMINI_API_KEY_{i}")
            if key:
                keys.append(key)
        
        if not keys:
            raise ValueError("âŒ GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        logger.info(f"ğŸ”‘ {len(keys)}ê°œì˜ Gemini API í‚¤ ë¡œë“œ ì™„ë£Œ")
        return keys
    
    def _initialize_llm_instances(self):
        """ê° API í‚¤ì— ëŒ€í•´ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        for key in self.api_keys:
            try:
                self.llm_instances[key] = GoogleGenerativeAI(
                    model=self.model_type.value,
                    google_api_key=key,
                    temperature=self.temperature,
                    max_output_tokens=self.max_tokens
                )
                logger.debug(f"âœ… LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì„±ê³µ: {key[:10]}...")
            except Exception as e:
                logger.error(f"âŒ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {key[:10]}... - {e}")
                self.api_key_status[key].is_active = False
    
    def _get_next_api_key(self) -> str:
        """ë‹¤ìŒ ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤ ë°˜í™˜ (ë¡œë“œë°¸ëŸ°ì‹±)"""
        active_keys = [
            key for key, status in self.api_key_status.items()
            if status.is_active
        ]
        
        if not active_keys:
            raise RuntimeError("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì„±ê³µë¥  ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì„ íƒ
        weights = [
            max(0.1, self.api_key_status[key].success_rate) 
            for key in active_keys
        ]
        
        # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ëœë¤ ì„ íƒ
        selected_key = random.choices(active_keys, weights=weights)[0]
        
        return selected_key
    
    def _check_rate_limit(self, api_key: str) -> bool:
        """API í‚¤ì˜ ìš”ì²­ ì œí•œ í™•ì¸"""
        status = self.api_key_status[api_key]
        current_time = time.time()
        
        # 1ë¶„ ë‚´ ìš”ì²­ ìˆ˜ í™•ì¸
        recent_requests = [
            req for req in self.request_history
            if req.get('api_key') == api_key and 
               current_time - req.get('timestamp', 0) < 60
        ]
        
        if len(recent_requests) >= self.rpm_limit:
            logger.warning(f"âš ï¸ API í‚¤ {api_key[:10]}... RPM ì œí•œ ì´ˆê³¼")
            return False
        
        return True
    
    async def _execute_with_retry(
        self,
        prompt: str,
        api_key: str,
        **kwargs
    ) -> str:
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ ìš”ì²­ ì‹¤í–‰"""
        llm = self.llm_instances[api_key]
        status = self.api_key_status[api_key]
        
        for attempt in range(self.max_retries):
            try:
                start_time = time.time()
                
                # ìš”ì²­ ì‹¤í–‰
                response = await llm.ainvoke(prompt, **kwargs)
                
                # ì„±ê³µ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                execution_time = time.time() - start_time
                status.success_count += 1
                status.last_request_time = time.time()
                
                # ìš”ì²­ íˆìŠ¤í† ë¦¬ ì¶”ê°€
                self.request_history.append({
                    'api_key': api_key,
                    'timestamp': time.time(),
                    'execution_time': execution_time,
                    'success': True
                })
                
                logger.debug(f"âœ… ìš”ì²­ ì„±ê³µ: {api_key[:10]}... ({execution_time:.2f}s)")
                return response
                
            except Exception as e:
                status.error_count += 1
                logger.warning(f"âš ï¸ ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{self.max_retries}): {e}")
                
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(self.retry_delay * (2 ** attempt))  # ì§€ìˆ˜ ë°±ì˜¤í”„
                else:
                    # ìµœì¢… ì‹¤íŒ¨ ì‹œ API í‚¤ ë¹„í™œì„±í™” (ì¼ì‹œì )
                    status.is_active = False
                    logger.error(f"âŒ API í‚¤ {api_key[:10]}... ì¼ì‹œì  ë¹„í™œì„±í™”")
                    
                    # ìš”ì²­ íˆìŠ¤í† ë¦¬ ì¶”ê°€
                    self.request_history.append({
                        'api_key': api_key,
                        'timestamp': time.time(),
                        'success': False,
                        'error': str(e)
                    })
                    
                    raise e
    
    async def generate_content(
        self,
        prompt: str,
        **kwargs
    ) -> str:
        """
        ì»¨í…ì¸  ìƒì„± (ë©€í‹° API í‚¤ ì§€ì›)
        
        Args:
            prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
            **kwargs: ì¶”ê°€ ë§¤ê°œë³€ìˆ˜
            
        Returns:
            ìƒì„±ëœ ì»¨í…ì¸ 
        """
        # ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤ ì„ íƒ
        api_key = self._get_next_api_key()
        
        # ìš”ì²­ ì œí•œ í™•ì¸
        if not self._check_rate_limit(api_key):
            # ë‹¤ë¥¸ API í‚¤ ì‹œë„
            for key in self.api_keys:
                if key != api_key and self._check_rate_limit(key):
                    api_key = key
                    break
            else:
                raise RuntimeError("âŒ ëª¨ë“  API í‚¤ê°€ ìš”ì²­ ì œí•œì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
        
        # ìš”ì²­ ì‹¤í–‰
        return await self._execute_with_retry(prompt, api_key, **kwargs)
    
    def create_chain(self, prompt_template: str) -> LLMChain:
        """
        Langchain Chain ìƒì„±
        
        Args:
            prompt_template: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
            
        Returns:
            LLMChain ì¸ìŠ¤í„´ìŠ¤
        """
        # í˜„ì¬ í™œì„± API í‚¤ ì„ íƒ
        api_key = self._get_next_api_key()
        llm = self.llm_instances[api_key]
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
        prompt = PromptTemplate.from_template(prompt_template)
        
        # ì²´ì¸ ìƒì„±
        chain = LLMChain(
            llm=llm,
            prompt=prompt,
            output_parser=StrOutputParser()
        )
        
        return chain
    
    def create_runnable_chain(self, prompt_template: str):
        """
        Langchain Runnable Chain ìƒì„±
        
        Args:
            prompt_template: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
            
        Returns:
            Runnable Chain
        """
        # í˜„ì¬ í™œì„± API í‚¤ ì„ íƒ
        api_key = self._get_next_api_key()
        llm = self.llm_instances[api_key]
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
        prompt = PromptTemplate.from_template(prompt_template)
        
        # Runnable Chain ìƒì„±
        chain = (
            {"input": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        
        return chain
    
    def get_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°˜í™˜"""
        # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        self._update_metrics()
        
        return {
            "total_requests": self.metrics.total_requests,
            "successful_requests": self.metrics.successful_requests,
            "failed_requests": self.metrics.failed_requests,
            "success_rate": self.metrics.success_rate,
            "average_response_time": self.metrics.average_response_time,
            "requests_per_minute": self.metrics.requests_per_minute,
            "api_key_status": {
                key[:10] + "...": {
                    "is_active": status.is_active,
                    "success_rate": status.success_rate,
                    "request_count": status.request_count,
                    "error_count": status.error_count
                }
                for key, status in self.api_key_status.items()
            }
        }
    
    def _update_metrics(self):
        """ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        current_time = time.time()
        
        # ìµœê·¼ 1ë¶„ê°„ ìš”ì²­ í•„í„°ë§
        recent_requests = [
            req for req in self.request_history
            if current_time - req.get('timestamp', 0) < 60
        ]
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        self.metrics.total_requests = len(self.request_history)
        self.metrics.successful_requests = sum(
            1 for req in self.request_history if req.get('success', False)
        )
        self.metrics.failed_requests = self.metrics.total_requests - self.metrics.successful_requests
        self.metrics.requests_per_minute = len(recent_requests)
        
        # í‰ê·  ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        successful_requests = [
            req for req in self.request_history
            if req.get('success', False) and 'execution_time' in req
        ]
        
        if successful_requests:
            self.metrics.average_response_time = sum(
                req['execution_time'] for req in successful_requests
            ) / len(successful_requests)
    
    def reset_api_key_status(self):
        """API í‚¤ ìƒíƒœ ì´ˆê¸°í™” (ë³µêµ¬)"""
        for status in self.api_key_status.values():
            status.is_active = True
            status.error_count = 0
        
        logger.info("ğŸ”„ API í‚¤ ìƒíƒœ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def health_check(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
        health_status = {
            "status": "healthy",
            "active_keys": 0,
            "total_keys": len(self.api_keys),
            "key_details": {}
        }
        
        for key, status in self.api_key_status.items():
            key_id = key[:10] + "..."
            
            if status.is_active:
                health_status["active_keys"] += 1
                
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìš”ì²­
                try:
                    await self._execute_with_retry("í…ŒìŠ¤íŠ¸", key)
                    health_status["key_details"][key_id] = "healthy"
                except Exception as e:
                    health_status["key_details"][key_id] = f"error: {str(e)}"
                    status.is_active = False
            else:
                health_status["key_details"][key_id] = "inactive"
        
        # ì „ì²´ ìƒíƒœ ê²°ì •
        if health_status["active_keys"] == 0:
            health_status["status"] = "critical"
        elif health_status["active_keys"] < health_status["total_keys"] / 2:
            health_status["status"] = "warning"
        
        return health_status


# ì „ì—­ í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
_global_client: Optional[GeminiClient] = None


def get_gemini_client() -> GeminiClient:
    """ì „ì—­ Gemini í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_client
    
    if _global_client is None:
        _global_client = GeminiClient()
    
    return _global_client


# í¸ì˜ í•¨ìˆ˜ë“¤
async def generate_content(prompt: str, **kwargs) -> str:
    """í¸ì˜ í•¨ìˆ˜: ì»¨í…ì¸  ìƒì„±"""
    client = get_gemini_client()
    return await client.generate_content(prompt, **kwargs)


def create_chain(prompt_template: str) -> LLMChain:
    """í¸ì˜ í•¨ìˆ˜: Chain ìƒì„±"""
    client = get_gemini_client()
    return client.create_chain(prompt_template)


def create_runnable_chain(prompt_template: str):
    """í¸ì˜ í•¨ìˆ˜: Runnable Chain ìƒì„±"""
    client = get_gemini_client()
    return client.create_runnable_chain(prompt_template) 